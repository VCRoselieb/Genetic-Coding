{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageDraw\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random start of population\n",
    "#genomes as a list of gene sequences\n",
    "def initialise():\n",
    "    genomes =[]\n",
    "    for i in range(4):\n",
    "        genomes.append([np.random.randint(2, size=6)]) # add 5 randomly initialised indiviudals\n",
    "    \n",
    "    #entschachtelungsprozess\n",
    "    ent = [5]*len(genomes)\n",
    "    i = 0\n",
    "    for k in genomes:\n",
    "        for l in k:\n",
    "            \n",
    "            ent[i] =l\n",
    "            i = i+1\n",
    "    \n",
    "\n",
    "    return ent\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usefull functions\n",
    "\n",
    "def paring(mom,dad):\n",
    "    return(np.concatenate((mom[:3], dad[3:]), axis=0))\n",
    "\n",
    "def entschachteln(genos):\n",
    "    print(len(genos))\n",
    "    neue = [0,0]\n",
    "    i = 0\n",
    "    for k in genos:\n",
    "        for l in k:\n",
    "            print(l)\n",
    "            neue[i] =l\n",
    "            i = i+1\n",
    "\n",
    "    return (neue)\n",
    "\n",
    "def mutation(popul):\n",
    "    position = random.randrange(0, len(popul))\n",
    "    gencode = popul[position]\n",
    "    \n",
    "    decider = random.uniform(0, 1)\n",
    "    posi = 0\n",
    "    if decider < 0.1 :\n",
    "        print('Mutation')\n",
    "        posi = random.randrange(0, len(gencode))\n",
    "    \n",
    "        if gencode[posi] == 1:\n",
    "            gencode[posi] = 0\n",
    "        elif gencode[posi] == 0:\n",
    "            gencode[posi] = 1\n",
    "            \n",
    "    popul[position] = np.array(gencode)\n",
    "    \n",
    "    return popul\n",
    "\n",
    "''' A one to one replacement reproduction '''\n",
    "def selected(maxis, actualpop): \n",
    "    childs= []\n",
    "    maxis= np.array(maxis)\n",
    "    indices = maxis.argsort()[-4:][::-1]\n",
    "    print('Highest acc at')\n",
    "    print(indices)\n",
    "    parents= []\n",
    "    \n",
    "   \n",
    "    for ind in indices:    \n",
    "        parents.append(actualpop[ind])\n",
    "            \n",
    "    print('PPPPPP')\n",
    "    print(parents)\n",
    "    random.shuffle(parents)\n",
    "    f= 0\n",
    "    print(parents)\n",
    "    \n",
    "    #### was passiert ungerade zahl geht nicht und 50 eltern überleben\n",
    "    # 1 un2 2un3\n",
    "    while f < len(parents)-1: \n",
    "        mom = parents[f]\n",
    "        f= f +1\n",
    "        dad = parents[f]     \n",
    "        child = paring(mom,dad)\n",
    "        childs.append(child)\n",
    "        child = paring(dad,mom)\n",
    "        childs.append(child)\n",
    "        \n",
    "        \n",
    "    return childs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netzdurchlauf(acutalpop):\n",
    "    accuracy_of_population= []\n",
    "    \n",
    "    print('Aktuelle Population')\n",
    "    print(actualpop)\n",
    "    for element in actualpop:\n",
    "        batch_size = 128\n",
    "        num_classes = 10\n",
    "        epochs = 12\n",
    "\n",
    "        # input image dimensions\n",
    "        img_rows, img_cols = 28, 28\n",
    "\n",
    "        # the data, split between train and test sets\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "            x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "            input_shape = (1, img_rows, img_cols)\n",
    "        else:\n",
    "            x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "            x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "            input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "#             print('x_train shape:', x_train.shape)\n",
    "#             print(x_train.shape[0], 'train samples')\n",
    "#             print(x_test.shape[0], 'test samples')\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "        y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "            ########\n",
    "            \n",
    "            \n",
    "\n",
    "        for gen in element:\n",
    "            if (gen== 0):\n",
    "                model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "            elif (gen == 1):\n",
    "                model.add(Dense(128, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                          optimizer=keras.optimizers.Adadelta(),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "        model.fit(x_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      verbose=1,\n",
    "                      validation_data=(x_test, y_test))\n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "        accuracy_of_population.append(score[1])\n",
    "        \n",
    "    return (accuracy_of_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fortpflanzung (maxis, actualpop):\n",
    "    childs= []\n",
    "    maxis= np.array(maxis)\n",
    "    indices = maxis.argsort()[-2:][::-1] # hier verändern, wenn wir individuen anzahl erhöhen die erste zahl muss die hälfte der anzahl sein\n",
    "    print('Highest acc at')\n",
    "    print(indices)\n",
    "    parents= []\n",
    "    \n",
    "    \n",
    "    for ind in indices:    \n",
    "        parents.append(actualpop[ind])\n",
    "   \n",
    "    \n",
    "    print('PPPPPP')\n",
    "    print(parents)\n",
    "    random.shuffle(parents)\n",
    "    f= 0\n",
    "    print(parents)\n",
    "    #parents = entschachteln(parents)\n",
    "    \n",
    "    while f < len(parents): \n",
    "        print(f)\n",
    "        f = f -1\n",
    "        mom = parents[f]\n",
    "        f= f +1\n",
    "        print(f)\n",
    "        dad = parents[f]    \n",
    "        f= f +1\n",
    "        decider = random.uniform(0, 1) # decides random how the cross-over works\n",
    "        if decider < 0.5 :\n",
    "            child = paring(mom,dad)\n",
    "            childs.append(child)\n",
    "        else:\n",
    "            child = paring(dad,mom)\n",
    "            childs.append(child)\n",
    "\n",
    "    \n",
    "#         child = paring(mom,dad)\n",
    "#         childs.append(child)\n",
    "    \n",
    "#             child = paring(dad,mom)\n",
    "#             childs.append(child)\n",
    "\n",
    "#     while f < len(parents)-1: \n",
    "#         mom = parents[f]\n",
    "#         f= f +1\n",
    "#         dad = parents[f]     \n",
    "#         child = paring(mom,dad)\n",
    "#         childs.append(child)\n",
    "#         child = paring(dad,mom)\n",
    "#         childs.append(child)\n",
    "    print('CCCCCCC')\n",
    "    print(childs)\n",
    "    parents = np.concatenate((parents, childs), axis=0) \n",
    "    \n",
    "    print('OVERALL')\n",
    "    print(parents)\n",
    "    print(len(parents))\n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aktuelle Population\n",
      "[array([0, 0, 0, 0, 1, 1]), array([0, 0, 0, 1, 1, 0]), array([1, 1, 0, 0, 0, 0]), array([1, 0, 1, 0, 1, 1])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0920 13:37:51.609639 15840 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0920 13:37:51.622620 15840 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0920 13:37:51.624612 15840 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0920 13:37:51.651532 15840 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0920 13:37:51.654522 15840 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0920 13:37:51.661501 15840 deprecation.py:506] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0920 13:37:51.794159 15840 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0920 13:37:51.801126 15840 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0920 13:37:51.951725 15840 deprecation.py:323] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.3783 - acc: 0.8774 - val_loss: 0.0672 - val_acc: 0.9768\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0700 - acc: 0.9788 - val_loss: 0.0448 - val_acc: 0.9861\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0470 - acc: 0.9851 - val_loss: 0.0362 - val_acc: 0.9879\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0367 - acc: 0.9885 - val_loss: 0.0267 - val_acc: 0.9923\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0291 - acc: 0.9909 - val_loss: 0.0194 - val_acc: 0.9942\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.0242 - acc: 0.9928 - val_loss: 0.0217 - val_acc: 0.9932\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0214 - acc: 0.9935 - val_loss: 0.0194 - val_acc: 0.9946\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.0191 - acc: 0.9941 - val_loss: 0.0214 - val_acc: 0.9933\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0157 - acc: 0.9951 - val_loss: 0.0198 - val_acc: 0.9947\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0138 - acc: 0.9958 - val_loss: 0.0256 - val_acc: 0.9924\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0131 - acc: 0.9960 - val_loss: 0.0184 - val_acc: 0.9946\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0113 - acc: 0.9965 - val_loss: 0.0240 - val_acc: 0.9937\n",
      "Test loss: 0.023965450994987623\n",
      "Test accuracy: 0.9937\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.3840 - acc: 0.8762 - val_loss: 0.0558 - val_acc: 0.9815\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0797 - acc: 0.9757 - val_loss: 0.0669 - val_acc: 0.9779\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0541 - acc: 0.9831 - val_loss: 0.0278 - val_acc: 0.9901\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0411 - acc: 0.9876 - val_loss: 0.0237 - val_acc: 0.9919\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0343 - acc: 0.9899 - val_loss: 0.0340 - val_acc: 0.9902\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0287 - acc: 0.9910 - val_loss: 0.0229 - val_acc: 0.9929\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.0257 - acc: 0.9922 - val_loss: 0.0242 - val_acc: 0.9928\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0233 - acc: 0.9928 - val_loss: 0.0189 - val_acc: 0.9935\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0202 - acc: 0.9936 - val_loss: 0.0230 - val_acc: 0.9927\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0193 - acc: 0.9940 - val_loss: 0.0166 - val_acc: 0.9948\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0168 - acc: 0.9949 - val_loss: 0.0195 - val_acc: 0.9940\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0153 - acc: 0.9955 - val_loss: 0.0186 - val_acc: 0.9948\n",
      "Test loss: 0.01859946153665478\n",
      "Test accuracy: 0.9948\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.3512 - acc: 0.8879 - val_loss: 0.0889 - val_acc: 0.9703\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.0788 - acc: 0.9760 - val_loss: 0.0334 - val_acc: 0.9892\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0519 - acc: 0.9843 - val_loss: 0.0309 - val_acc: 0.9905\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0397 - acc: 0.9879 - val_loss: 0.0299 - val_acc: 0.9904\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.0331 - acc: 0.9901 - val_loss: 0.0225 - val_acc: 0.9929\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0272 - acc: 0.9916 - val_loss: 0.0213 - val_acc: 0.9934\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0241 - acc: 0.9924 - val_loss: 0.0222 - val_acc: 0.9934\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.0212 - acc: 0.9933 - val_loss: 0.0284 - val_acc: 0.9917\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.0200 - acc: 0.9935 - val_loss: 0.0215 - val_acc: 0.9934\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.0172 - acc: 0.9943 - val_loss: 0.0235 - val_acc: 0.9931\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0145 - acc: 0.9954 - val_loss: 0.0211 - val_acc: 0.9934\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.0143 - acc: 0.9954 - val_loss: 0.0231 - val_acc: 0.9931\n",
      "Test loss: 0.023105697293815137\n",
      "Test accuracy: 0.9931\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.2684 - acc: 0.9161 - val_loss: 0.0746 - val_acc: 0.9775\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0708 - acc: 0.9787 - val_loss: 0.0437 - val_acc: 0.9848\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0521 - acc: 0.9838 - val_loss: 0.0322 - val_acc: 0.9893\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0424 - acc: 0.9873 - val_loss: 0.0330 - val_acc: 0.9886\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: 0.0352 - acc: 0.9888 - val_loss: 0.0276 - val_acc: 0.9905\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.0322 - acc: 0.9899 - val_loss: 0.0290 - val_acc: 0.9902\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.0277 - acc: 0.9913 - val_loss: 0.0248 - val_acc: 0.9912\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.0235 - acc: 0.9926 - val_loss: 0.0244 - val_acc: 0.9916\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0228 - acc: 0.9924 - val_loss: 0.0230 - val_acc: 0.9930\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0200 - acc: 0.9937 - val_loss: 0.0245 - val_acc: 0.9925\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0175 - acc: 0.9945 - val_loss: 0.0211 - val_acc: 0.9933\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0164 - acc: 0.9947 - val_loss: 0.0210 - val_acc: 0.9930\n",
      "Test loss: 0.020977800093947008\n",
      "Test accuracy: 0.993\n",
      " Accuracy of populationnummer: \n",
      "1\n",
      "[0.9937, 0.9948, 0.9931, 0.993]\n",
      "highest accuracy of populationnummer: \n",
      "1\n",
      "0.9948\n",
      "Highest acc at\n",
      "[1 0]\n",
      "PPPPPP\n",
      "[array([0, 0, 0, 1, 1, 0]), array([0, 0, 0, 0, 1, 1])]\n",
      "[array([0, 0, 0, 0, 1, 1]), array([0, 0, 0, 1, 1, 0])]\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "CCCCCCC\n",
      "[array([0, 0, 0, 0, 1, 1]), array([0, 0, 0, 1, 1, 0])]\n",
      "OVERALL\n",
      "[[0 0 0 0 1 1]\n",
      " [0 0 0 1 1 0]\n",
      " [0 0 0 0 1 1]\n",
      " [0 0 0 1 1 0]]\n",
      "4\n",
      "Aktuelle Population\n",
      "[[0 0 0 0 1 1]\n",
      " [0 0 0 1 1 0]\n",
      " [0 0 0 0 1 1]\n",
      " [0 0 0 1 1 0]]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.3761 - acc: 0.8770 - val_loss: 0.0489 - val_acc: 0.9844\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.0746 - acc: 0.9778 - val_loss: 0.0570 - val_acc: 0.9828\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0504 - acc: 0.9847 - val_loss: 0.0305 - val_acc: 0.9900\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.0391 - acc: 0.9879 - val_loss: 0.0291 - val_acc: 0.9903\n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0310 - acc: 0.9908 - val_loss: 0.0261 - val_acc: 0.9910\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0268 - acc: 0.9914 - val_loss: 0.0265 - val_acc: 0.9920\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0226 - acc: 0.9930 - val_loss: 0.0204 - val_acc: 0.9934\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.0193 - acc: 0.9940 - val_loss: 0.0300 - val_acc: 0.9915\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.0177 - acc: 0.9943 - val_loss: 0.0251 - val_acc: 0.9915\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0155 - acc: 0.9953 - val_loss: 0.0210 - val_acc: 0.9931\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.0137 - acc: 0.9957 - val_loss: 0.0233 - val_acc: 0.9932\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0111 - acc: 0.9965 - val_loss: 0.0217 - val_acc: 0.9936\n",
      "Test loss: 0.021683169538700715\n",
      "Test accuracy: 0.9936\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.3797 - acc: 0.8749 - val_loss: 0.0872 - val_acc: 0.9726\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0814 - acc: 0.9754 - val_loss: 0.0443 - val_acc: 0.9870\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.0530 - acc: 0.9838 - val_loss: 0.0392 - val_acc: 0.9885\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.0412 - acc: 0.9873 - val_loss: 0.0266 - val_acc: 0.9918\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0353 - acc: 0.9893 - val_loss: 0.0219 - val_acc: 0.9936\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0300 - acc: 0.9906 - val_loss: 0.0228 - val_acc: 0.9928\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0260 - acc: 0.9922 - val_loss: 0.0243 - val_acc: 0.9921\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0230 - acc: 0.9933 - val_loss: 0.0235 - val_acc: 0.9929\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.0199 - acc: 0.9937 - val_loss: 0.0203 - val_acc: 0.9941\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.0190 - acc: 0.9942 - val_loss: 0.0188 - val_acc: 0.9939\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0162 - acc: 0.9948 - val_loss: 0.0192 - val_acc: 0.9943\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0152 - acc: 0.9952 - val_loss: 0.0223 - val_acc: 0.9934\n",
      "Test loss: 0.022315948965215285\n",
      "Test accuracy: 0.9934\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.3794 - acc: 0.8775 - val_loss: 0.0846 - val_acc: 0.9729\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.0743 - acc: 0.9774 - val_loss: 0.0339 - val_acc: 0.9894\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0491 - acc: 0.9851 - val_loss: 0.0310 - val_acc: 0.9900\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0399 - acc: 0.9880 - val_loss: 0.0289 - val_acc: 0.9909\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0309 - acc: 0.9907 - val_loss: 0.0233 - val_acc: 0.9928\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.0267 - acc: 0.9916 - val_loss: 0.0279 - val_acc: 0.9910: 0s - loss: 0.0271 - acc: \n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 8s 130us/step - loss: 0.0231 - acc: 0.9928 - val_loss: 0.0233 - val_acc: 0.9925\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.0204 - acc: 0.9937 - val_loss: 0.0228 - val_acc: 0.9929\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0178 - acc: 0.9946 - val_loss: 0.0211 - val_acc: 0.9924\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.0150 - acc: 0.9951 - val_loss: 0.0224 - val_acc: 0.9930\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0137 - acc: 0.9957 - val_loss: 0.0207 - val_acc: 0.9942\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0123 - acc: 0.9963 - val_loss: 0.0255 - val_acc: 0.9928\n",
      "Test loss: 0.025501387057331886\n",
      "Test accuracy: 0.9928\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.3668 - acc: 0.8822 - val_loss: 0.0736 - val_acc: 0.9785\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0850 - acc: 0.9746 - val_loss: 0.0332 - val_acc: 0.9888\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0556 - acc: 0.9835 - val_loss: 0.0410 - val_acc: 0.9867\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0408 - acc: 0.9877 - val_loss: 0.0293 - val_acc: 0.9903\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0361 - acc: 0.9883 - val_loss: 0.0230 - val_acc: 0.9926\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0299 - acc: 0.9910 - val_loss: 0.0223 - val_acc: 0.9925\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0259 - acc: 0.9921 - val_loss: 0.0240 - val_acc: 0.9923\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0217 - acc: 0.9930 - val_loss: 0.0197 - val_acc: 0.9938\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0200 - acc: 0.9937 - val_loss: 0.0192 - val_acc: 0.9943\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0177 - acc: 0.9946 - val_loss: 0.0209 - val_acc: 0.9933\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0160 - acc: 0.9950 - val_loss: 0.0185 - val_acc: 0.9942\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0151 - acc: 0.9953 - val_loss: 0.0200 - val_acc: 0.9936\n",
      "Test loss: 0.020003335939691533\n",
      "Test accuracy: 0.9936\n",
      " Accuracy of populationnummer: \n",
      "2\n",
      "[0.9936, 0.9934, 0.9928, 0.9936]\n",
      "highest accuracy of populationnummer: \n",
      "2\n",
      "0.9936\n",
      "Highest acc at\n",
      "[3 0]\n",
      "PPPPPP\n",
      "[array([0, 0, 0, 1, 1, 0]), array([0, 0, 0, 0, 1, 1])]\n",
      "[array([0, 0, 0, 0, 1, 1]), array([0, 0, 0, 1, 1, 0])]\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "CCCCCCC\n",
      "[array([0, 0, 0, 0, 1, 1]), array([0, 0, 0, 1, 1, 0])]\n",
      "OVERALL\n",
      "[[0 0 0 0 1 1]\n",
      " [0 0 0 1 1 0]\n",
      " [0 0 0 0 1 1]\n",
      " [0 0 0 1 1 0]]\n",
      "4\n",
      "Aktuelle Population\n",
      "[[0 0 0 0 1 1]\n",
      " [0 0 0 1 1 0]\n",
      " [0 0 0 0 1 1]\n",
      " [0 0 0 1 1 0]]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.3732 - acc: 0.8790 - val_loss: 0.0992 - val_acc: 0.9706\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0731 - acc: 0.9777 - val_loss: 0.0391 - val_acc: 0.9872\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.0490 - acc: 0.9852 - val_loss: 0.0260 - val_acc: 0.9918\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0384 - acc: 0.9877 - val_loss: 0.0247 - val_acc: 0.9923\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0306 - acc: 0.9903 - val_loss: 0.0288 - val_acc: 0.9912\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0265 - acc: 0.9921 - val_loss: 0.0239 - val_acc: 0.9924\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0235 - acc: 0.9929 - val_loss: 0.0198 - val_acc: 0.9938\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 8s 134us/step - loss: 0.0205 - acc: 0.9937 - val_loss: 0.0254 - val_acc: 0.9924\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.0173 - acc: 0.9948 - val_loss: 0.0211 - val_acc: 0.9931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0159 - acc: 0.9948 - val_loss: 0.0204 - val_acc: 0.9944\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0139 - acc: 0.9955 - val_loss: 0.0321 - val_acc: 0.9914\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0121 - acc: 0.9962 - val_loss: 0.0181 - val_acc: 0.9946\n",
      "Test loss: 0.018118250372678814\n",
      "Test accuracy: 0.9946\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.3813 - acc: 0.8764 - val_loss: 0.0664 - val_acc: 0.9785\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0838 - acc: 0.9744 - val_loss: 0.0485 - val_acc: 0.9857\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0556 - acc: 0.9833 - val_loss: 0.0323 - val_acc: 0.9895\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0420 - acc: 0.9871 - val_loss: 0.0273 - val_acc: 0.9908\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0346 - acc: 0.9896 - val_loss: 0.0246 - val_acc: 0.9926\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.0296 - acc: 0.9908 - val_loss: 0.0212 - val_acc: 0.9936\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0262 - acc: 0.9919 - val_loss: 0.0215 - val_acc: 0.9941\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.0224 - acc: 0.9934 - val_loss: 0.0216 - val_acc: 0.9935\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0200 - acc: 0.9938 - val_loss: 0.0197 - val_acc: 0.9938\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0176 - acc: 0.9943 - val_loss: 0.0237 - val_acc: 0.9922\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0160 - acc: 0.9948 - val_loss: 0.0223 - val_acc: 0.9940\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0156 - acc: 0.9951 - val_loss: 0.0199 - val_acc: 0.9940\n",
      "Test loss: 0.019899739505693834\n",
      "Test accuracy: 0.994\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.3820 - acc: 0.8773 - val_loss: 0.0647 - val_acc: 0.9790\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0741 - acc: 0.9772 - val_loss: 0.0412 - val_acc: 0.9874\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0488 - acc: 0.9851 - val_loss: 0.0259 - val_acc: 0.9913\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0370 - acc: 0.9889 - val_loss: 0.0272 - val_acc: 0.9913\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0314 - acc: 0.9906 - val_loss: 0.0231 - val_acc: 0.9922\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0260 - acc: 0.9918 - val_loss: 0.0219 - val_acc: 0.9937\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0223 - acc: 0.9932 - val_loss: 0.0304 - val_acc: 0.9905\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.0191 - acc: 0.9941 - val_loss: 0.0229 - val_acc: 0.9932\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0156 - acc: 0.9949 - val_loss: 0.0178 - val_acc: 0.9947\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0145 - acc: 0.9953 - val_loss: 0.0210 - val_acc: 0.9936\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.0141 - acc: 0.9958 - val_loss: 0.0240 - val_acc: 0.9935\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0125 - acc: 0.9962 - val_loss: 0.0231 - val_acc: 0.9936\n",
      "Test loss: 0.023050021302065987\n",
      "Test accuracy: 0.9936\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.3862 - acc: 0.8740 - val_loss: 0.0622 - val_acc: 0.9789\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0835 - acc: 0.9740 - val_loss: 0.0428 - val_acc: 0.9855\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0555 - acc: 0.9826 - val_loss: 0.0303 - val_acc: 0.9908\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0422 - acc: 0.9869 - val_loss: 0.0259 - val_acc: 0.9921\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0354 - acc: 0.9886 - val_loss: 0.0237 - val_acc: 0.9930\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0299 - acc: 0.9911 - val_loss: 0.0255 - val_acc: 0.9915\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0258 - acc: 0.9916 - val_loss: 0.0217 - val_acc: 0.9932\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0232 - acc: 0.9924 - val_loss: 0.0277 - val_acc: 0.9917\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.0202 - acc: 0.9935 - val_loss: 0.0209 - val_acc: 0.9934\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0187 - acc: 0.9941 - val_loss: 0.0201 - val_acc: 0.9945\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0169 - acc: 0.9947 - val_loss: 0.0243 - val_acc: 0.9937\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0155 - acc: 0.9952 - val_loss: 0.0207 - val_acc: 0.9938\n",
      "Test loss: 0.020681277607477386\n",
      "Test accuracy: 0.9938\n",
      " Accuracy of populationnummer: \n",
      "3\n",
      "[0.9946, 0.994, 0.9936, 0.9938]\n",
      "highest accuracy of populationnummer: \n",
      "3\n",
      "0.9946\n",
      "Highest acc at\n",
      "[0 1]\n",
      "PPPPPP\n",
      "[array([0, 0, 0, 0, 1, 1]), array([0, 0, 0, 1, 1, 0])]\n",
      "[array([0, 0, 0, 0, 1, 1]), array([0, 0, 0, 1, 1, 0])]\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "CCCCCCC\n",
      "[array([0, 0, 0, 1, 1, 0]), array([0, 0, 0, 0, 1, 1])]\n",
      "OVERALL\n",
      "[[0 0 0 0 1 1]\n",
      " [0 0 0 1 1 0]\n",
      " [0 0 0 1 1 0]\n",
      " [0 0 0 0 1 1]]\n",
      "4\n",
      "Mutation\n",
      "Aktuelle Population\n",
      "[[0 0 0 0 1 1]\n",
      " [1 0 0 1 1 0]\n",
      " [0 0 0 1 1 0]\n",
      " [0 0 0 0 1 1]]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.3617 - acc: 0.8827 - val_loss: 0.0628 - val_acc: 0.9795\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0751 - acc: 0.9770 - val_loss: 0.0459 - val_acc: 0.9860\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0499 - acc: 0.9853 - val_loss: 0.0363 - val_acc: 0.9874\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0386 - acc: 0.9884 - val_loss: 0.0309 - val_acc: 0.9910\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0312 - acc: 0.9908 - val_loss: 0.0261 - val_acc: 0.9914\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0264 - acc: 0.9916 - val_loss: 0.0221 - val_acc: 0.9923\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 9s 144us/step - loss: 0.0226 - acc: 0.9932 - val_loss: 0.0258 - val_acc: 0.9917\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.0198 - acc: 0.9939 - val_loss: 0.0151 - val_acc: 0.9946\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0172 - acc: 0.9950 - val_loss: 0.0177 - val_acc: 0.9951\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.0155 - acc: 0.9953 - val_loss: 0.0188 - val_acc: 0.9947\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.0129 - acc: 0.9958 - val_loss: 0.0198 - val_acc: 0.9943\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.0123 - acc: 0.9962 - val_loss: 0.0208 - val_acc: 0.9939\n",
      "Test loss: 0.020835834155037675\n",
      "Test accuracy: 0.9939\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 12s 205us/step - loss: 0.3229 - acc: 0.8975 - val_loss: 0.0593 - val_acc: 0.9820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.0813 - acc: 0.9749 - val_loss: 0.0413 - val_acc: 0.9856\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.0549 - acc: 0.9830 - val_loss: 0.0317 - val_acc: 0.9895\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.0442 - acc: 0.9864 - val_loss: 0.0422 - val_acc: 0.9871\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0380 - acc: 0.9880 - val_loss: 0.0266 - val_acc: 0.9922\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.0328 - acc: 0.9899 - val_loss: 0.0213 - val_acc: 0.9932\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0279 - acc: 0.9912 - val_loss: 0.0207 - val_acc: 0.9930\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0256 - acc: 0.9919 - val_loss: 0.0209 - val_acc: 0.9937\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 10s 163us/step - loss: 0.0232 - acc: 0.9924 - val_loss: 0.0198 - val_acc: 0.9937\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0204 - acc: 0.9935 - val_loss: 0.0245 - val_acc: 0.9922\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.0187 - acc: 0.9941 - val_loss: 0.0209 - val_acc: 0.9929\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0185 - acc: 0.9946 - val_loss: 0.0199 - val_acc: 0.9938\n",
      "Test loss: 0.019873646203335375\n",
      "Test accuracy: 0.9938\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.3871 - acc: 0.8744 - val_loss: 0.0789 - val_acc: 0.9766\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.0806 - acc: 0.9754 - val_loss: 0.0319 - val_acc: 0.9910\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0537 - acc: 0.9840 - val_loss: 0.0289 - val_acc: 0.9910\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0415 - acc: 0.9873 - val_loss: 0.0282 - val_acc: 0.9918\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0343 - acc: 0.9896 - val_loss: 0.0285 - val_acc: 0.9913\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0305 - acc: 0.9908 - val_loss: 0.0230 - val_acc: 0.9931\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.0244 - acc: 0.9924 - val_loss: 0.0233 - val_acc: 0.9925\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0238 - acc: 0.9927 - val_loss: 0.0209 - val_acc: 0.9932\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.0205 - acc: 0.9935 - val_loss: 0.0198 - val_acc: 0.9935\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0185 - acc: 0.9944 - val_loss: 0.0301 - val_acc: 0.9906\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.0172 - acc: 0.9942 - val_loss: 0.0208 - val_acc: 0.9928\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 9s 150us/step - loss: 0.0143 - acc: 0.9956 - val_loss: 0.0225 - val_acc: 0.9934\n",
      "Test loss: 0.022509214642799272\n",
      "Test accuracy: 0.9934\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.3405 - acc: 0.8911 - val_loss: 0.0705 - val_acc: 0.9761\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0715 - acc: 0.9779 - val_loss: 0.0377 - val_acc: 0.9872\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.0496 - acc: 0.9845 - val_loss: 0.0273 - val_acc: 0.9919\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0383 - acc: 0.9885 - val_loss: 0.0322 - val_acc: 0.9905\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.0310 - acc: 0.9904 - val_loss: 0.0480 - val_acc: 0.9851\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 8s 136us/step - loss: 0.0268 - acc: 0.9923 - val_loss: 0.0255 - val_acc: 0.9920\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0217 - acc: 0.9936 - val_loss: 0.0208 - val_acc: 0.9937\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0204 - acc: 0.9938 - val_loss: 0.0216 - val_acc: 0.9933\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0174 - acc: 0.9946 - val_loss: 0.0225 - val_acc: 0.9928\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0156 - acc: 0.9953 - val_loss: 0.0208 - val_acc: 0.9934\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 9s 142us/step - loss: 0.0140 - acc: 0.9956 - val_loss: 0.0255 - val_acc: 0.9930\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.0109 - acc: 0.9965 - val_loss: 0.0259 - val_acc: 0.9931\n",
      "Test loss: 0.025851921410293882\n",
      "Test accuracy: 0.9931\n",
      " Accuracy of populationnummer: \n",
      "4\n",
      "[0.9939, 0.9938, 0.9934, 0.9931]\n",
      "highest accuracy of populationnummer: \n",
      "4\n",
      "0.9939\n",
      "Highest acc at\n",
      "[0 1]\n",
      "PPPPPP\n",
      "[array([0, 0, 0, 0, 1, 1]), array([1, 0, 0, 1, 1, 0])]\n",
      "[array([0, 0, 0, 0, 1, 1]), array([1, 0, 0, 1, 1, 0])]\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "CCCCCCC\n",
      "[array([0, 0, 0, 1, 1, 0]), array([0, 0, 0, 1, 1, 0])]\n",
      "OVERALL\n",
      "[[0 0 0 0 1 1]\n",
      " [1 0 0 1 1 0]\n",
      " [0 0 0 1 1 0]\n",
      " [0 0 0 1 1 0]]\n",
      "4\n",
      "Aktuelle Population\n",
      "[[0 0 0 0 1 1]\n",
      " [1 0 0 1 1 0]\n",
      " [0 0 0 1 1 0]\n",
      " [0 0 0 1 1 0]]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.3610 - acc: 0.8824 - val_loss: 0.0642 - val_acc: 0.9791\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 8s 139us/step - loss: 0.0699 - acc: 0.9782 - val_loss: 0.0498 - val_acc: 0.9844\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.0489 - acc: 0.9849 - val_loss: 0.0359 - val_acc: 0.9880\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0364 - acc: 0.9889 - val_loss: 0.0306 - val_acc: 0.9899\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.0299 - acc: 0.9905 - val_loss: 0.0220 - val_acc: 0.9934\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 9s 145us/step - loss: 0.0252 - acc: 0.9921 - val_loss: 0.0237 - val_acc: 0.9930\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 8s 142us/step - loss: 0.0222 - acc: 0.9932 - val_loss: 0.0313 - val_acc: 0.9915\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 8s 142us/step - loss: 0.0201 - acc: 0.9936 - val_loss: 0.0214 - val_acc: 0.9935\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 8s 138us/step - loss: 0.0173 - acc: 0.9945 - val_loss: 0.0186 - val_acc: 0.9944\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0155 - acc: 0.9947 - val_loss: 0.0229 - val_acc: 0.9930\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 8s 141us/step - loss: 0.0137 - acc: 0.9960 - val_loss: 0.0254 - val_acc: 0.9918\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 8s 142us/step - loss: 0.0123 - acc: 0.9960 - val_loss: 0.0278 - val_acc: 0.9924\n",
      "Test loss: 0.027822042082847293\n",
      "Test accuracy: 0.9924\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 209us/step - loss: 0.3251 - acc: 0.8968 - val_loss: 0.0723 - val_acc: 0.9813\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0752 - acc: 0.9770 - val_loss: 0.0523 - val_acc: 0.9849\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 1412s 24ms/step - loss: 0.0519 - acc: 0.9843 - val_loss: 0.0307 - val_acc: 0.9895\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0409 - acc: 0.9874 - val_loss: 0.0266 - val_acc: 0.9917\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.0341 - acc: 0.9891 - val_loss: 0.0252 - val_acc: 0.9917\n",
      "Epoch 6/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.0300 - acc: 0.9906 - val_loss: 0.0248 - val_acc: 0.9918\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0254 - acc: 0.9921 - val_loss: 0.0206 - val_acc: 0.9932\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.0234 - acc: 0.9926 - val_loss: 0.0238 - val_acc: 0.9917\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0211 - acc: 0.9935 - val_loss: 0.0225 - val_acc: 0.9932\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0190 - acc: 0.9940 - val_loss: 0.0213 - val_acc: 0.9932\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0180 - acc: 0.9944 - val_loss: 0.0203 - val_acc: 0.9944\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 13s 208us/step - loss: 0.0167 - acc: 0.9945 - val_loss: 0.0205 - val_acc: 0.9932\n",
      "Test loss: 0.02045985019147738\n",
      "Test accuracy: 0.9932\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.4099 - acc: 0.8660 - val_loss: 0.0617 - val_acc: 0.9800\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0781 - acc: 0.9766 - val_loss: 0.0387 - val_acc: 0.9876\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.0528 - acc: 0.9845 - val_loss: 0.0286 - val_acc: 0.9915\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 12s 195us/step - loss: 0.0402 - acc: 0.9877 - val_loss: 0.0316 - val_acc: 0.9894\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0339 - acc: 0.9893 - val_loss: 0.0351 - val_acc: 0.9885\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 0.0283 - acc: 0.9912 - val_loss: 0.0213 - val_acc: 0.9939\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0265 - acc: 0.9916 - val_loss: 0.0295 - val_acc: 0.9899\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0229 - acc: 0.9928 - val_loss: 0.0258 - val_acc: 0.9910\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0203 - acc: 0.9933 - val_loss: 0.0236 - val_acc: 0.9919\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0186 - acc: 0.9943 - val_loss: 0.0234 - val_acc: 0.9927\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: 0.0169 - acc: 0.9947 - val_loss: 0.0207 - val_acc: 0.9942\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.0150 - acc: 0.9953 - val_loss: 0.0240 - val_acc: 0.9934\n",
      "Test loss: 0.024002524685627122\n",
      "Test accuracy: 0.9934\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.3801 - acc: 0.8768 - val_loss: 0.0706 - val_acc: 0.9790\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.0853 - acc: 0.9739 - val_loss: 0.0387 - val_acc: 0.9883\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.0553 - acc: 0.9833 - val_loss: 0.0426 - val_acc: 0.9864\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.0424 - acc: 0.9867 - val_loss: 0.0292 - val_acc: 0.9902\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.0348 - acc: 0.9892 - val_loss: 0.0229 - val_acc: 0.9922\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.0296 - acc: 0.9906 - val_loss: 0.0271 - val_acc: 0.9898\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.0247 - acc: 0.9926 - val_loss: 0.0233 - val_acc: 0.9927\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.0231 - acc: 0.9928 - val_loss: 0.0190 - val_acc: 0.9938\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.0207 - acc: 0.9940 - val_loss: 0.0234 - val_acc: 0.9930\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0178 - acc: 0.9944 - val_loss: 0.0196 - val_acc: 0.9938\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.0156 - acc: 0.9950 - val_loss: 0.0206 - val_acc: 0.9941\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0146 - acc: 0.9953 - val_loss: 0.0259 - val_acc: 0.9916\n",
      "Test loss: 0.025932101580147717\n",
      "Test accuracy: 0.9916\n",
      " Accuracy of populationnummer: \n",
      "5\n",
      "[0.9924, 0.9932, 0.9934, 0.9916]\n",
      "highest accuracy of populationnummer: \n",
      "5\n",
      "0.9934\n",
      "Highest acc at\n",
      "[2 1]\n",
      "PPPPPP\n",
      "[array([0, 0, 0, 1, 1, 0]), array([1, 0, 0, 1, 1, 0])]\n",
      "[array([0, 0, 0, 1, 1, 0]), array([1, 0, 0, 1, 1, 0])]\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "CCCCCCC\n",
      "[array([1, 0, 0, 1, 1, 0]), array([0, 0, 0, 1, 1, 0])]\n",
      "OVERALL\n",
      "[[0 0 0 1 1 0]\n",
      " [1 0 0 1 1 0]\n",
      " [1 0 0 1 1 0]\n",
      " [0 0 0 1 1 0]]\n",
      "4\n",
      "Aktuelle Population\n",
      "[[0 0 0 1 1 0]\n",
      " [1 0 0 1 1 0]\n",
      " [1 0 0 1 1 0]\n",
      " [0 0 0 1 1 0]]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 0.3658 - acc: 0.8803 - val_loss: 0.0800 - val_acc: 0.9748\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.0795 - acc: 0.9763 - val_loss: 0.0419 - val_acc: 0.9875\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.0560 - acc: 0.9827 - val_loss: 0.0329 - val_acc: 0.9891\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0420 - acc: 0.9872 - val_loss: 0.0269 - val_acc: 0.9912\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0341 - acc: 0.9898 - val_loss: 0.0255 - val_acc: 0.9921\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0305 - acc: 0.9905 - val_loss: 0.0278 - val_acc: 0.9910\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0268 - acc: 0.9917 - val_loss: 0.0214 - val_acc: 0.9935\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0231 - acc: 0.9927 - val_loss: 0.0256 - val_acc: 0.9932\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.0205 - acc: 0.9931 - val_loss: 0.0234 - val_acc: 0.9934\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0177 - acc: 0.9942 - val_loss: 0.0206 - val_acc: 0.9939\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0152 - acc: 0.9954 - val_loss: 0.0313 - val_acc: 0.9896\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.0150 - acc: 0.9951 - val_loss: 0.0248 - val_acc: 0.9934\n",
      "Test loss: 0.024770608936611097\n",
      "Test accuracy: 0.9934\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 14s 241us/step - loss: 0.3224 - acc: 0.8983 - val_loss: 0.0666 - val_acc: 0.9778\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.0788 - acc: 0.9761 - val_loss: 0.0461 - val_acc: 0.9852\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 0.0554 - acc: 0.9827 - val_loss: 0.0349 - val_acc: 0.9886\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.0446 - acc: 0.9862 - val_loss: 0.0267 - val_acc: 0.9910\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 13s 216us/step - loss: 0.0375 - acc: 0.9884 - val_loss: 0.0227 - val_acc: 0.9931\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 0.0339 - acc: 0.9889 - val_loss: 0.0197 - val_acc: 0.9937\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 13s 214us/step - loss: 0.0282 - acc: 0.9913 - val_loss: 0.0188 - val_acc: 0.9935\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.0259 - acc: 0.9916 - val_loss: 0.0210 - val_acc: 0.9935\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 0.0233 - acc: 0.9926 - val_loss: 0.0192 - val_acc: 0.9940\n",
      "Epoch 10/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 12s 200us/step - loss: 0.0208 - acc: 0.9932 - val_loss: 0.0203 - val_acc: 0.9936\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0195 - acc: 0.9938 - val_loss: 0.0217 - val_acc: 0.9929\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 12s 205us/step - loss: 0.0179 - acc: 0.9940 - val_loss: 0.0226 - val_acc: 0.9937\n",
      "Test loss: 0.02262539964258831\n",
      "Test accuracy: 0.9937\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 15s 250us/step - loss: 0.3223 - acc: 0.8986 - val_loss: 0.0636 - val_acc: 0.9803\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.0815 - acc: 0.9752 - val_loss: 0.0424 - val_acc: 0.9855\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 12s 206us/step - loss: 0.0566 - acc: 0.9824 - val_loss: 0.0324 - val_acc: 0.9889\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0440 - acc: 0.9861 - val_loss: 0.0242 - val_acc: 0.9922\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.0361 - acc: 0.9890 - val_loss: 0.0249 - val_acc: 0.9922\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 13s 219us/step - loss: 0.0308 - acc: 0.9905 - val_loss: 0.0281 - val_acc: 0.9912\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.0283 - acc: 0.9907 - val_loss: 0.0236 - val_acc: 0.9930\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 13s 216us/step - loss: 0.0246 - acc: 0.9923 - val_loss: 0.0198 - val_acc: 0.9936\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.0224 - acc: 0.9930 - val_loss: 0.0206 - val_acc: 0.9936\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 13s 216us/step - loss: 0.0202 - acc: 0.9937 - val_loss: 0.0198 - val_acc: 0.9944\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 13s 219us/step - loss: 0.0188 - acc: 0.9938 - val_loss: 0.0199 - val_acc: 0.9944\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 12s 208us/step - loss: 0.0185 - acc: 0.9941 - val_loss: 0.0217 - val_acc: 0.9930\n",
      "Test loss: 0.0217156762487386\n",
      "Test accuracy: 0.993\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.3704 - acc: 0.8802 - val_loss: 0.0630 - val_acc: 0.9790\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0817 - acc: 0.9755 - val_loss: 0.0359 - val_acc: 0.9870\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.0523 - acc: 0.9840 - val_loss: 0.0344 - val_acc: 0.9885\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0410 - acc: 0.9871 - val_loss: 0.0262 - val_acc: 0.9917\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.0327 - acc: 0.9901 - val_loss: 0.0329 - val_acc: 0.9905\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.0299 - acc: 0.9906 - val_loss: 0.0247 - val_acc: 0.9920\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0253 - acc: 0.9921 - val_loss: 0.0196 - val_acc: 0.9944\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0222 - acc: 0.9932 - val_loss: 0.0215 - val_acc: 0.9933\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0199 - acc: 0.9941 - val_loss: 0.0239 - val_acc: 0.9924\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0191 - acc: 0.9942 - val_loss: 0.0193 - val_acc: 0.9940\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.0164 - acc: 0.9946 - val_loss: 0.0202 - val_acc: 0.9940\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 10s 168us/step - loss: 0.0151 - acc: 0.9952 - val_loss: 0.0220 - val_acc: 0.9938\n",
      "Test loss: 0.02200063016963104\n",
      "Test accuracy: 0.9938\n",
      " Accuracy of populationnummer: \n",
      "6\n",
      "[0.9934, 0.9937, 0.993, 0.9938]\n",
      "highest accuracy of populationnummer: \n",
      "6\n",
      "0.9938\n",
      "Highest acc at\n",
      "[3 1]\n",
      "PPPPPP\n",
      "[array([0, 0, 0, 1, 1, 0]), array([1, 0, 0, 1, 1, 0])]\n",
      "[array([0, 0, 0, 1, 1, 0]), array([1, 0, 0, 1, 1, 0])]\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "CCCCCCC\n",
      "[array([1, 0, 0, 1, 1, 0]), array([0, 0, 0, 1, 1, 0])]\n",
      "OVERALL\n",
      "[[0 0 0 1 1 0]\n",
      " [1 0 0 1 1 0]\n",
      " [1 0 0 1 1 0]\n",
      " [0 0 0 1 1 0]]\n",
      "4\n",
      "The highest accuracies in population\n",
      "[[1, 0.9948, 0.99365], [2, 0.9936, 0.99335], [3, 0.9946, 0.994], [4, 0.9939, 0.99355], [5, 0.9934, 0.9926499999999999], [6, 0.9938, 0.9934749999999999]]\n",
      "Wartezeit: \n",
      "71.45548001925151\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hauptmethode\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "DURCHGÄNGE = 6 # variable to clarify number of generations\n",
    "actualpop= initialise()\n",
    "\n",
    "#actualpop= entschachteln(actualpop)\n",
    "evalaccuris = []\n",
    "saved = actualpop\n",
    "i = 1\n",
    "    \n",
    "while i < DURCHGÄNGE+1:\n",
    "    \n",
    "    nummer = i\n",
    "    accuri = netzdurchlauf(actualpop)\n",
    "    print(' Accuracy of populationnummer: ')\n",
    "    print( nummer)\n",
    "    print(accuri)\n",
    "    print('highest accuracy of populationnummer: ')\n",
    "    print( nummer)\n",
    "    print(np.amax(accuri))\n",
    "    #print(actualpop)\n",
    "    actualpop = fortpflanzung(accuri, actualpop)\n",
    "    actualpop = mutation(actualpop)\n",
    "   \n",
    "    evalaccuris.append([i ,np.amax(accuri), sum(accuri)/len(accuri)])\n",
    "    i = i+1\n",
    "    \n",
    "    \n",
    "    \n",
    "print('The highest accuracies in population')\n",
    "print(evalaccuris)  \n",
    "\n",
    "print('Wartezeit: ')\n",
    "end = time.time()\n",
    "seconds = end - start\n",
    "minutes = seconds / 60\n",
    "print(minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.99, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de5zddX3n8df7XOaWK5CIaFiUmpZN2xQwoFu73Kw1lK6I2Iqtt6rFumJtu2q1tdLFB1W7rF20LMpaXNiqiKg17aLAIrc+RCWgKFeTptqESxMJkutczjmf/eP3PTO/OXNmcn6TOTPJzPv5eMxjzvldznx/Icw737siAjMzs06V5roAZmZ2eHFwmJlZIQ4OMzMrxMFhZmaFODjMzKwQB4eZmRXS1eCQdLWk7ZIemOS8JH1c0mZJ35d0cu7cGyVtSl9vzB1/oaQfpHs+LkndfAYzMxuv2zWO/w2sn+L82cDq9HUhcCWApCOBi4EXAacCF0s6It1zZbq2ed9Un29mZjOsq8EREXcCO6e45Fzg2sh8C1gu6Rjg5cAtEbEzIp4GbgHWp3NLI+LuyGYuXgu8spvPYGZm41Xm+Oc/F9iae78tHZvq+LY2xyeQdCFZzYRFixa98IQTTpi5UpuZLQD33nvvTyJiZevxuQ6Odv0TMY3jEw9GXAVcBbBu3brYuHHjdMtoZrYgSfpxu+NzPapqG3Bs7v0q4PEDHF/V5riZmc2SuQ6ODcAb0uiqFwPPRMQTwE3Ar0k6InWK/xpwUzq3W9KL02iqNwBfnbPSm5ktQF1tqpL0eeAMYIWkbWQjpaoAEfFJ4Ebg14HNwD7gd9O5nZI+BNyTPuqSiGh2sr+dbLRWP/C19GVmZrNEC2FZdfdxmJkVJ+neiFjXenyum6rMzOww4+AwM7NCHBxmZlaIg8PMzApxcJiZWSEODjMzK8TBYWZmhTg4zMysEAeHmZkV4uAwM7NCHBxmZlaIg8PMzApxcJiZWSEODjMzK8TBYWZmhTg4zMysEAeHmZkV4uAwM7NCHBxmZlaIg8PMzApxcJiZWSEODjMzK8TBYWZmhTg4zMysEAeHmZkV4uAwM7NCHBxmZlaIg8PMzApxcJiZWSEODjMzK8TBYWZmhTg4zMysEAeHmZkV4uAwM7NCHBxmZlZIV4ND0npJj0raLOl9bc4fJ+lWSd+XdLukVblzH5X0QPp6Te74SyXdJ+l7kv5J0gu6+QxmZjZe14JDUhm4AjgbWAO8VtKalssuA66NiLXAJcCH073nACcDJwIvAt4jaWm650rgdyLiROBzwAe69QxmZjZRN2scpwKbI2JLRAwD1wHntlyzBrg1vb4td34NcEdE1CJiL3A/sD6dC6AZIsuAx7tUfjMza6ObwfFcYGvu/bZ0LO9+4Pz0+jxgiaSj0vGzJQ1IWgGcCRybrnsrcKOkbcDrgY+0++GSLpS0UdLGHTt2zMgDmZlZd4NDbY5Fy/t3A6dL+i5wOvAYUIuIm4EbgW8CnwfuBmrpnj8Cfj0iVgGfAT7W7odHxFURsS4i1q1cufKgH8bMzDLdDI5tjNUSAFbR0qwUEY9HxKsi4iTgz9KxZ9L3SyPixIh4GVkIbZK0EviliPh2+ogvAL/cxWcwM7MW3QyOe4DVkp4vqQe4ANiQv0DSCknNMrwfuDodL6cmKyStBdYCNwNPA8sk/Wy652XAw118BjMza1Hp1gdHRE3SRcBNQBm4OiIelHQJsDEiNgBnAB+WFMCdwDvS7VXgLkkAu4DXRUQNQNLvAV+S1CALkjd36xnMzGwiRbR2O8w/69ati40bN851MczMDiuS7o2Ida3HPXPczMwKcXCYmVkhDg4zMyvEwWFmZoU4OMzMrBAHh5mZFeLgMDOzQhwcZmZWiIPDzMwKcXCYmVkhDg4zMyvEwWFmZoU4OMzMrBAHh5mZFeLgMDOzQhwcZmZWiIPDzMwKcXCYmVkhDg4zMyvEwWFmZoU4OMzMrBAHh5mZFeLgMDOzQhwcZmZWiIPDzMwKOWBwSLq1k2NmZrYwVCY7IakPGABWSDoCUDq1FHjOLJTNzMwOQZMGB/A24A/JQuJexoJjF3BFl8tlZmaHqEmDIyIuBy6X9M6I+MQslsnMzA5hnXSOPylpCYCkD0j6sqSTu1wuMzM7RHUSHH8eEbsl/QrwcuAa4MruFsvMzA5VnQRHPX0/B7gyIr4K9HSvSGZmdijrJDgek/Qp4LeAGyX1dnifmZnNQ50EwG8BNwHrI+KnwJHAe7paKjMzO2QdMDgiYh+wHfiVdKgGbOrkwyWtl/SopM2S3tfm/HGSbpX0fUm3S1qVO/dRSQ+kr9fkjkvSpZJ+KOlhSX/QSVnMzGxmTDWPAwBJFwPrgJ8DPgNUgb8DXnKA+8pk8z1eBmwD7pG0ISIeyl12GXBtRFwj6Szgw8DrJZ0DnAycCPQCd0j6WkTsAt4EHAucEBENSc8q8sBmZnZwOmmqOg94BbAXICIeB5Z0cN+pwOaI2BIRw8B1wLkt16wBmsuX3JY7vwa4IyJqEbEXuB9Yn869HbgkIhqpPNs7KIuZmc2QToJjOCICCABJizr87OcCW3Pvt6VjefcD56fX5wFLJB2Vjp8taUDSCuBMsloGwM8Ar5G0UdLXJK1u98MlXZiu2bhjx44Oi2xmZgfSSXBcn0ZVLZf0e8D/A/5XB/epzbFoef9u4HRJ3wVOBx4DahFxM3Aj8E3g88DdZH0rkDVdDUbEulSOq9v98Ii4KiLWRcS6lStXdlBcMzPrxAH7OCLiMkkvI1uj6ueAD0bELR189jbGagkAq4DHWz77ceBVAJIWA+dHxDPp3KXApenc5xjrkN8GfCm9/gpZv4uZmc2SAwYHQAqKW1Kz0VMdfvY9wGpJzyerSVwA/Hb+gvR5O1N/xftJtYfUsb48Ip6StBZYC9ycbvt74Kx07enADzssj5mZzYBJm6okvTgNkf2ypJMkPQA8APybpPWT3dcUETXgIrI5IA8D10fEg5IukfSKdNkZwKOSfggcTaphkI3cukvSQ8BVwOvS5wF8BDhf0g/IRmG9teAzm5nZQVDW793mhLQR+FNgGdkv77Mj4luSTgA+HxEnzV4xD866deti48aNc10MM7PDiqR7U3/yOFN1jlci4uaI+CLwZER8CyAiHulWIc3M7NA3VXA0cq/3t5xrX00xM7N5b6rO8V+StItsWG1/ek1639f1kpmZ2SFpqh0Ay7NZEDMzOzx4eXQzMyvEwWFmZoU4OMzMrJADBoekiyQdMRuFMTOzQ18nNY5nk+2lcX3amKnd4oVmZrZAdLID4AeA1cDfkm2itEnSX0r6mS6XzczMDkEd9XGk/TieTF814AjgBkl/1cWyzZiRerBj9xDP7Bth71CNoVqdRsNzGM3MpqOTrWP/AHgj8BPg08B7ImJEUolsqfP3dreIB68Rwe7BkQnHyyVRKZeopu+VsqiWsu+VknCrnJnZRJ0sq74CeFVE/Dh/MO33/RvdKdbsqDeCeqPO0CTnq+VmiJSollO4lES1XKJccqiY2cLUSXDcCOxsvpG0BFgTEd+OiIe7VrJDwEi9wUgdoD7hXEnKaigpTCrlFC4pZFxbMbP5qpPguBI4Ofd+b5tjC04jguFaMFxrtD1faTZ55Zq/8iFjZna46iQ4FLlNO1ITVUc7By5ktUaDWgOY2LWCpNEmr2awlFO/Srnk/hUzO7R1EgBbUgf5len9fwa2dK9I819EMFIPRurtayuQddxnIVIaDZN8uFRLJUruZzGzOdBJcPw+8HHgA2T7cNwKXNjNQh0KvrNlJ9fds5Undu3nmKX9XHDKsZx6/JGz9vOzjvtgmMnDpaQULuXxIVNteW9mNpMOGBwRsR24YBbKcsj4zpadXP6NTVRKYmlfhaf2DnH5NzbxLlbPangcSCOCRj1SB357zWaxZq2lUs7VYHLHzMw61ck8jj7gLcDPk9vAKSLe3MVyzanr7tlKpST6q9mWJP3VMvtH6lx3z9ZDKjg6MdYsNvk1kigrawqr5pvIUtNYM2Tc72Jm0FlT1f8BHgFeDlwC/A4wr4fhPrFrP0v7xv/R9FVLPLmrdQfd+SEiqEVQazDpnBZgXJjkayvu1DdbWDoJjhdExG9KOjcirpH0OeCmbhdsLh2ztJ+n9g6N1jgABkcaPHtp/xyWau41R4odKFyqlazG0lMee+25LWbzRyfB0RxQ+lNJv0C2XtXzulaiQ8AFpxzL5d/YxP6ROn3VEoMjDWqN4IJTjp3roh3yao0GtWFonTTZ2tdSzn3lR47N9kix2x/Zzqfu3MLWp/dx7BEDvO204znjhGfNahnMDjedBMdVaT+ODwAbgMXAn3e1VHPs1OOP5F2s5rp7tvLkrv08ew5GVc03nfS1wCQjxTR+KHJZMxMwtz+ynQ9ueJBqWSzvr7J99yAf3PAgl4DDw2wKUwZHWshwV0Q8DdwJHD8rpToEnHr8kQ6KOdDJSDHIAqYkIZG+REkgsu+ILGBSyPRWSvRWSuOayz515xaqZTHQk/1vMNBTYd9wjU/ducXBYTaFKYMjzRK/CLh+lsrTFc/sG+a2R7azuK/C4t7sa0l67aGoh6dGBI0otjS+lM1x6SmXqJZL/HjnXpb3V4mI0UDpr5bZ9vS+bhTZbN7opKnqFknvBr5Atk4VABGxc/JbDi3/tnuID/3f9gPB+qollvRWU6iUWdxbHQ2VdkHTPLakr0J/tewO38NItKwv9qzFfWODIFJtZXCkxtFL+3hm30g2PDkFjf87m43pJDia8zXekTsWHEbNVtVyiWX9VfYM1ai3bOA0ONJgcGSIHXumGivUXkmMhsmS3iqLe8ssyr1e0lcdFzTNEGoeq7q2M6daB0HsH6lTawSvPnkVT+0d//ehWi7RU8lGivVWs++urdpCpShY3T8c/eKJJ8dXb7mTiGBwpMGeoRq7B0fYPVRjz2CNvUO10de7h9L7wRp7msfS6/0HangvqK9SGhcsi0ZrN1nwLO6rZu9Hw6mSgqnCQI9rOzOhubTMdAZBlEuip5I1e1XTMORm570nTNp8IOneiFjXeryTmeNvaHc8Iq6diYLNJkn095Tp7ymzcklv4fvrjUjhMjIaKnuGcgGTe72nNXza1XZqDQb3DPOTPcOFy1ISuaBpaUYbfV2dtJnNtZ3MwQyCqDeC/cN19rfZrwXGZuSXSmOjxSYbjlxKHfxmh4NOmqpOyb3uA14K3AccdsFxsMolsWygyrKBauF7I4LBWmMsbEYDqM6ewZEJQdMMpN2pRrR3ePwvp0bA7lQbeuKZ4s/SWylNCJMJ/Tm9FRb3TezzGegpU/IvuQNqzsifYp3KcSQhGBcwlXJp0vBx0Nhc6WSRw3fm30taRrYMiRUgZWtf9VcPoraTwiTflDYWNmNNb3va1HxqLbWdoVqDodowT+09uNrO4lwzWrvBBItyr5ekpreeims77UQEAR0NR4b2c14qLfNd3GRm3TCdDZn2AatnuiA2tXJJLOuvsqx/erWdoVpjXA1m92AtFzTjazytfTxT1XamoyfVdpbkgqXZx7Mk19y2qKXpbUlvlYFe13aaisx5yTeNlTQ2S7+SO9Y8b3YgnfRx/APZKCqAErCGw3xex0Ijib5qmb5qmRWLp1fb2Ts0vvks33ezN1+7SbWfPUN1dqcmuJH6+NrOcK3BztowO6dR2xGMBsyi1ma1SZreRmtEvRV6c+uPLRSdBkxTfnJlqZQ1nylNqGxOtBy9ptR8nQ1nlsbeNydf2vzTSY3jstzrGvDjiNjWpfLYIahcEkv7qyydRm0HYGikPmHgQLs+nd3N2s9gPYXQCHuHxv+2Cxi9fzqqZY1rNsuHyoSmtpZQWtRbWRD/Ih83ufIgBxLmg6bcWrtJfTfNY671HD46CY5/BZ6IiEEASf2SnhcRPzrQjZLWA5cDZeDTEfGRlvPHAVcDK4GdwOuaoSTpo8A56dIPRcQXWu79BPC7EbG4g2ewOdRbLdN7ELWdfcMtQdMyTLrdCLdm7ac52a9ppB48vW+Ep/e12Qy+A4t6yi21mslHrrWGUeuSJwtBRFAPqNN5jScfNsqFTrMm0+5aMRZC7t/pvk6C44vAL+fe19OxU9pfnpFUBq4AXgZsA+6RtCEiHspddhlwbVqu/Szgw8DrJZ0DnAycCPQCd0j6WkTsSp+9DljeyQPa4a1cEkv6qizpq3LMsuL3D9ca7B7Mai67h0bG9fHk+3yaQZM/v3eoRussp73DdfYO1/m3KReXb69Z28nXdBa1mbfTGjxLFlBtB8aHzcFSazNaKRdKMKHpDTEhtJRbA83Nb5lOgqMSEaON0RExLKmng/tOBTZHxBYASdcB5wL54FgD/FF6fRvw97njd0REDahJuh9YD1yfAum/Ab8NnNdBOWwB66mUOGpxL0dNo17aiGDfcD3VbtLcnfzw6dbgGRxf8xnqQm1n0VS1m7R0zoSmt74KfbnaTnPS4xO79nPMPF/5uRlCo920MzSHd0KwpD9b0QyX3CKbUwTSaF8RzHggRQTNFsdI78deQ6Q/k/wc8Pzxqf6h0klw7JD0iojYACDpXOAnHdz3XGBr7v024EUt19wPnE/WnHUesETSUen4xZI+BgwAZzIWOBcBGyLiiamqoZIuBC4EeM4q76NhxZU0VkN49rK+A9/QYjiNZBtrOhvrv8n6cmoThlDnR7a1jKAere1s3128tlMpKS3qKZ7ZP0JZ2TDeTdt386EbH+K0F6zk3z9nSWp6K4/rA1q8gGo7nWpkv2FnpFaUlw8UGFv5eVwITPJLH8YHxMFa3Dd5PHQSHL8PfFbS36T324C2s8lbtPub1vpE7wb+RtKbyJZtfwyoRcTNkk4BvgnsAO4mq3k8B/hN4IwD/fCIuAq4CrIlRzoor9mM6qmUOLLSw5GLOqmgj9eIbFZ6PljG9+mMtO3j2T1UY+9gjcGW2k6tEfx0/1hNZ4TIhrokX3vwSb724JOTlmegpzzlYILWuTz54OlbgH070zVTgdTtWmUnEwD/GXixpMVka1vt7vCztwH5f+qvAh5v+ezHgVcBpM8/PyKeSecuBS5N5z4HbAJOAl4AbE5/EQckbY6IF3RYJrPDQkliUerbYGnx+4drDfYOT6zNXH7rJnrKohHQaAT1COqNbJOtlUt6R5vdWms7+4br7DvI2k7rKLUJc3fGrdc2dp0XkyzmO1t2cvk3NlEpiaV9FZ7aO8Tl39jEu1g9Y+HRyTyOvwT+KiJ+mt4fAfyXiPjAAW69B1gt6flkNYkLyPol8p+9AtgZEQ3g/WQjrJod68sj4ilJa4G1wM2pz+PZufv3ODTMJuqplOip9HDEwPjazj/e/8TYUvLJ/pE6Ry3q5WOv+SUga+rYl2o7zTk6E0a0jYbRyIQRbZPVdvI1niLGb30w+TYH7ebyLMStD667ZyuVkkb/G/dXy+wfqXPdPVtnLziAsyPiT5tvIuJpSb9OtpXspCKiljaBuolsOO7VEfGgpEuAjanP5Azgw5KCrKmquXR7Fbgr/QffRTZMd3oD981sVOtS8oMjDWqN4IJTxhoHlK/tTMNIvTF+9YF82LRtdhubs7N7cGRCbWcmtj6YMG9nkqBpbXo7HGs7T+zaz9KW/om+aoknd+2fsZ/Ryd+MsqTeiBiCbB4H2RDZA4qIG4EbW459MPf6BuCGNvcNko2sOtDnew6HWQGnHn8k72L1tJeS70S1XGL5QA/LB4r37UQE+0fqLSsRjG19MNmyOM0wat36oBGwa7DGrmkuj9Pc+qA5ZHpRLoTy2xy0qwXN1dYHxyztn1CrHBxp8Oyl/TP2MzoJjr8DbpX0GbLO7TezAFfGXQgW0jDNhexglpLvNinbA36gp8J0dn2v1Rujc3Y6WalgXHNcF7Y+aN3uIL/+2pK+ats+nub76W590Emt8mB1tJFTmgH+q2QjpW6OiJtmrASzoLmRk00u36GW/8v2rrNmrkPN7FDWuvXB6NydNsOm2+2/s2945jd6WzTJKLax2k37lQse2PYM12/cdlC1ysV9FY5e2j+9jZwAIuLrwNcBJL1E0hUR8Y4D3DZv5Sf7NGeVkl6PXTO2v0LzuFpGKLfWYidUaiecn1jtHfczpyjrZNc0T99w3zZ6KyUGerK/EtVymX3DNb503zbOXntMoXJ3+jPHzk/9XK3a/Vsn2gxfbDfGfeI1Ma68+UlSk/68Sf6t1VqGqX5+RNCIse/N9aEi2t0XE/4lbDNvJrc+mLiZ28i0N3p76iA3ets7XOP6e7dy4wNPTD2YIDcA4UBbH3QUHJJOBF4LvAb4F+DLhZ9kDlXL4uilYxO4mr/sm78vxpYVaJ4f+0Wi3D2t5+aTJ3cNsry/Ou75FvdWeHLX4LSWcreZ12i0i8fxOvn7GmkIbjOw6o2g1ggajUgBloVV5M5HkO5xgE1mJrY+mNCcNrra9MQJot3c+qC3UmLJdCYASvpZsiG0rwWeAr5A1rR15rRKMoeaY+JtcsceMcD23YOjNQ7IhmmuOmJgDktleTO1JIXSrPHpyoJkYsCMBlE9BVGMBVIt3TMXDoe+u/zWBwdT29k71L4207rVQeuot9atD4ZqDYamqOlM9dv0EeAu4D9FxOb0cH80xfV2GHvbacfzwQ0Psm+4Njrue6QevO204+e6aHaIyZYfKR489UaMftUaDRoNqDUa2bEIavWY8VrNbEyGOxQcbG1nuNaYsPbaSL3Bmz7a/p6pguN8shrHbZK+DlzHdP622GHhjBOexSXAp+7cwran97HqiAHedtrxnHHCdMa2mE00fq+NyTfUilRTaTahTRo2HfT7zMZkuMOdpLZbH0xrraqI+ArwFUmLgFeSrWJ7tKQrga9ExM0zVnI7JJxxwrMcFDbnJFEtiwNt1tjsq6m39NPkQ+fJXfsntNXP9GS4haiTtar2Ap8lW+jwSLJFBt8HODjMbM40+2qm+iX2/BWL2b57kP5qiebq6vuGa6w6YoAlfVXqjWCoVveotYIKzTCJiJ0R8amIOKtbBTIzmylvO+14RurZbHQBg7U69YCLznwBK5f08uxlfRx31CL+3ZEDHL20j+UDPfT3lEeXNbf2PNTIzOatTvvuKuUSlXKJRbkBTSP1RvZVC0Ya2evhWsO1ExwcZjbPTbfvrlouZct+tCy5VW9ko5CGavVs2OpIg1qj0f5D5ikHh5lZAeWS6O8p098z1ntfqzeyEGkGykhjXk+WdHCYmR2kdk1d+VrJcPqaL2Hi4DAz64JsM60SS3LHxjVxpTCZqxn1B8PBYWY2SyYLk+HU8T5Uqx8WHfAODjOzOdQMk/z2eLV6g8Fag6GRsdrJoVQzcXCYmR1iKuUSi8slFqfFWSOC4dQBPziSdb6P1OduJJeDw8zsECeJ3kqZ3kqZpX3ZQoaNRoyN4prlYcEODjOzw1CpzbDg/ByT4Voj7eQ582Hi4DAzmyfazTFprsc1NDI2z+RgO98dHGZm81i5JAZ6KgzkZsDXGzG6pEr2emzp+k6WrHdwmJktMNneKNmOg+0caJtiB4eZmY1zoG2KCy2rbmZm5uAwM7NCHBxmZlaIg8PMzApxcJiZWSEODjMzK8TBYWZmhTg4zMysEAeHmZkV4uAwM7NCuhocktZLelTSZknva3P+OEm3Svq+pNslrcqd+6ikB9LXa3LHP5s+8wFJV0uqdvMZzMxsvK4Fh6QycAVwNrAGeK2kNS2XXQZcGxFrgUuAD6d7zwFOBk4EXgS8R9LSdM9ngROAXwT6gbd26xnMzGyibtY4TgU2R8SWiBgGrgPObblmDXBren1b7vwa4I6IqEXEXuB+YD1ARNwYCfAdYBVmZjZruhkczwW25t5vS8fy7gfOT6/PA5ZIOiodP1vSgKQVwJnAsfkbUxPV64Gvt/vhki6UtFHSxh07dhz0w5iZWaabwdFuXd7WJd7fDZwu6bvA6cBjQC0ibgZuBL4JfB64G6i13Ps/gTsj4q52PzwiroqIdRGxbuXKlQfxGGZmltfN4NjG+FrCKuDx/AUR8XhEvCoiTgL+LB17Jn2/NCJOjIiXkYXQpuZ9ki4GVgJ/3MXym5lZG90MjnuA1ZKeL6kHuADYkL9A0gpJzTK8H7g6HS+nJiskrQXWAjen928FXg68NiJmfhd2MzObUteCIyJqwEXATcDDwPUR8aCkSyS9Il12BvCopB8CRwOXpuNV4C5JDwFXAa9LnwfwyXTt3ZK+J+mD3XoGMzObSNngpPlt3bp1sXHjxrkuhpnZYUXSvRGxrvW4Z46bmVkhDg4zMyvEwWFmZoU4OMzMrBAHh5mZFeLgMDOzQhwcZmZWiIPDzMwKcXCYmVkhDg4zMyvEwWFmZoU4OMzMrBAHh5mZFeLgMDOzQhwcZmZWiIPDzMwKcXCYmVkhDg4zMyvEwWFmZoU4OMzMrBAHh5mZFeLgMDOzQhwcZmZWiIPDzMwKcXCYmVkhDg4zMyvEwWFmZoU4OMzMrBAHh5mZFeLgMDOzQhwcZmZWiIPDzMwKcXCYmVkhDg4zMyvEwWFmZoV0NTgkrZf0qKTNkt7X5vxxkm6V9H1Jt0talTv3UUkPpK/X5I4/X9K3JW2S9AVJPd18BjMzG69rwSGpDFwBnA2sAV4raU3LZZcB10bEWuAS4MPp3nOAk4ETgRcB75G0NN3zUeCvI2I18DTwlm49g5mZTdTNGsepwOaI2BIRw8B1wLkt16wBbk2vb8udXwPcERG1iNgL3A+slyTgLOCGdN01wCu7+AxmZtai0sXPfi6wNfd+G1ntIe9+4HzgcuA8YImko9LxiyV9DBgAzgQeAo4CfhoRtdxnPrfdD5d0IXBhertH0qPTfI4VwE+mee/hys+8MCy0Z15ozwsH/8zHtTvYzeBQm2PR8v7dwN9IehNwJ/AYUIuImyWdAnwT2AHcDdQ6/MzsYMRVwFXTK/oYSRsjYt3Bfs7hxM+8MCy0Z15ozwvde+ZuNlVtA47NvV8FPJ6/ICIej4hXRcRJwJ+lY8+k75dGxIkR8TKywNhElpzLJVUm+0wzM+uubgbHPcDqNAqqB7gA2JC/QNIKSc0yvB+4Oh0vpyYrJK0F1gI3R0SQ9YW8Ot3zRuCrXXwGMzNr0bXgSP0QFwE3AQ8D10fEg5IukVxw5IEAAAVzSURBVPSKdNkZwKOSfggcDVyajleBuyQ9RNbc9Lpcv8afAH8saTNZn8ffdusZkoNu7joM+ZkXhoX2zAvteaFLz6zsH/FmZmad8cxxMzMrxMFhZmaFODgmIelqSdslPTDXZZktko6VdJukhyU9KOldc12mbpLUJ+k7ku5Pz/tf57pMsyUNQPmupH+c67LMBkk/kvQDSd+TtHGuyzMbJC2XdIOkR9L/0/9hxj7bfRztSToN2EO2JMovzHV5ZoOkY4BjIuI+SUuAe4FXRsRDc1y0rkgrESyKiD2SqsA/Ae+KiG/NcdG6TtIfA+uApRHxG3Ndnm6T9CNgXUQsmAmAkq4B7oqIT6eRrQMR8dOZ+GzXOCYREXcCO+e6HLMpIp6IiPvS691ko+HazsyfDyKzJ72tpq95/y+ptJjoOcCn57os1h1pbb/TSKNOI2J4pkIDHBw2CUnPA04Cvj23Jemu1GTzPWA7cEtEzOvnTf4H8F6gMdcFmUUB3Czp3rQc0Xx3PNmqG59JTZKflrRopj7cwWETSFoMfAn4w4jYNdfl6aaIqEfEiWSrEJwqaV43S0r6DWB7RNw712WZZS+JiJPJVut+R2qKns8qZCuMX5lW5tgLTNjaYrocHDZOauv/EvDZiPjyXJdntqRq/O3A+jkuSre9BHhFavO/DjhL0t/NbZG6LyIeT9+3A18hW717PtsGbMvVoG8gC5IZ4eCwUamz+G+BhyPiY3Ndnm6TtFLS8vS6H/hV4JG5LVV3RcT7I2JVRDyPbBmgb0TE6+a4WF0laVEa7EFqrvk1YF6PloyIJ4Gtkn4uHXop2QrjM6Kbq+Me1iR9nmxJlBWStgEXR0S3lzeZay8BXg/8ILX7A/xpRNw4h2XqpmOAa9KmYyWyZXEWxPDUBeZo4CvZv4uoAJ+LiK/PbZFmxTuBz6YRVVuA352pD/ZwXDMzK8RNVWZmVoiDw8zMCnFwmJlZIQ4OMzMrxMFhZmaFODjM2pB0tKTPSdqSlqm4W9J5c1SWMyT9cu7970t6w1yUxQw8j8NsgjQR8u+BayLit9Ox44BXTHnjwf3MSm575FZnkK3U/E2AiPhkt8ph1gnP4zBrIemlwAcj4vQ258rAR8h+mfcCV0TEpySdAfwF8BPgF8iWpH9dRISkFwIfAxan82+KiCck3U4WBi8BNgA/BD4A9ABPAb8D9APfAupki9a9k2wW8J6IuEzSicAngQHgn4E3R8TT6bO/DZwJLAfeEhF3zdyfki1kbqoym+jngfsmOfcW4JmIOAU4Bfg9Sc9P504C/hBYQ7Y66UvS2l+fAF4dES8ErgYuzX3e8og4PSL+O9l+IC9Oi9JdB7w3In5EFgx/HREntvnlfy3wJxGxFvgBcHHuXCUiTk1luhizGeKmKrMDkHQF8CvAMPBjYK2kV6fTy4DV6dx3ImJbuud7wPOAn5LVQG5JS16UgSdyH/+F3OtVwBfShlo9wL8coFzLyILnjnToGuCLuUuai1Tem8piNiMcHGYTPQic33wTEe+QtALYCPwr8M6IuCl/Q2qqGsodqpP9/yXgwYiYbNvOvbnXnwA+FhEbck1fB6NZnmZZzGaEm6rMJvoG0Cfp7bljA+n7TcDbUxMUkn72ABvkPAqsbO73LKkq6ecnuXYZ8Fh6/cbc8d3AktaLI+IZ4GlJ/zEdej1wR+t1ZjPN/woxa5E6tF8J/LWk95J1Su8F/oSsKeh5wH1p9NUO4JVTfNZwatb6eGpaqpDtwPdgm8v/AviipMfIOsSbfSf/ANwg6VyyzvG8NwKflDTADK+AajYZj6oyM7NC3FRlZmaFODjMzKwQB4eZmRXi4DAzs0IcHGZmVoiDw8zMCnFwmJlZIf8fSdZNgFHKH+YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualisation\n",
    "df = pd.DataFrame(data =evalaccuris)\n",
    "df.rename(columns={0: 'Generation',1:'Accuracy Best',2: 'Accuracy mean'}, inplace=True)\n",
    "df\n",
    "sns.regplot(x=df['Generation'],y=df['Accuracy Best'])\n",
    "plt.ylim(0.990, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.99, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualisation von dem durchschnitt der accuracy\n",
    "\n",
    "sns.regplot(x=df['Generation'],y=df['Accuracy mean'])\n",
    "plt.ylim(0.990, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    " \n",
    "ax1 =sns.regplot(x=df['Generation'],y=df['Accuracy Best'])\n",
    "ax0 =sns.regplot(x=df['Generation'],y=df['Accuracy mean'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wievielschwnakung \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
