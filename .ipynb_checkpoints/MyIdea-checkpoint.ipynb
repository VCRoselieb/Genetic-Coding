{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageDraw\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random start of population\n",
    "#genomes as a list of gene sequences\n",
    "def initialise():\n",
    "    genomes =[]\n",
    "    for i in range(2):\n",
    "        genomes.append([np.random.randint(2, size=6)]) # add 5 randomly initialised indiviudals\n",
    "    \n",
    "    return genomes\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usefull functions\n",
    "\n",
    "def paring(mom,dad):\n",
    "    return(np.concatenate((mom[:3], dad[3:]), axis=0))\n",
    "\n",
    "\n",
    "def mutation(popul):\n",
    "    position = random.randrange(0, len(popul))\n",
    "    gencode = popul[position]\n",
    "    \n",
    "    decider = random.uniform(0, 1)\n",
    "    posi = 0\n",
    "    if decider < 0.2 :\n",
    "        print('Mutation')\n",
    "        posi = random.randrange(0, len(gencode))\n",
    "    \n",
    "        if gencode[posi] == 1:\n",
    "            gencode[posi] = 0\n",
    "        elif gencode[posi] == 0:\n",
    "            gencode[posi] = 1\n",
    "            \n",
    "    popul[position] = np.array(gencode)\n",
    "    \n",
    "    return popul\n",
    "\n",
    "\n",
    "def selected(maxis, actualpop):\n",
    "    childs= []\n",
    "    maxis= np.array(maxis)\n",
    "    indices = maxis.argsort()[-2:][::-1]\n",
    "    print('Highest acc at')\n",
    "    print(indices)\n",
    "    parents= []\n",
    "    \n",
    "  \n",
    "    for ind in indices:\n",
    "    \n",
    "        for k in actualpop[ind]:\n",
    "         \n",
    "         \n",
    "            k=list(k)\n",
    "            parents.append(k)\n",
    "   \n",
    "    \n",
    "    random.shuffle(parents)\n",
    "    f= 0\n",
    "    while f < len(parents)-1: \n",
    "        mom = parents[f]\n",
    "        f= f +1\n",
    "        dad = parents[f]     \n",
    "        child = paring(mom,dad)\n",
    "        childs.append(child)\n",
    "        child = paring(dad,mom)\n",
    "        childs.append(child)\n",
    "        \n",
    "        \n",
    "    return childs\n",
    "\n",
    "\n",
    "def selected2(maxis, actualpop):\n",
    "    childs= []\n",
    "    maxis= np.array(maxis)\n",
    "    indices = maxis.argsort()[-2:][::-1]\n",
    "    print('Highest acc at')\n",
    "    print(indices)\n",
    "    parents= []\n",
    "    \n",
    "   \n",
    "    for ind in indices:    \n",
    "        parents.append(actualpop[ind])\n",
    "            \n",
    "   \n",
    "    \n",
    "    random.shuffle(parents)\n",
    "    f= 0\n",
    "    while f < len(parents)-1: \n",
    "        mom = parents[f]\n",
    "        f= f +1\n",
    "        dad = parents[f]     \n",
    "        child = paring(mom,dad)\n",
    "        childs.append(child)\n",
    "        child = paring(dad,mom)\n",
    "        childs.append(child)\n",
    "        \n",
    "        \n",
    "    return childs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netzdurchlauf(acutalpop):\n",
    "    accuracy_of_population= []\n",
    "    \n",
    "    print('Aktuelle Population')\n",
    "    print(actualpop)\n",
    "    for element in actualpop:\n",
    "        batch_size = 128\n",
    "        num_classes = 10\n",
    "        epochs = 12\n",
    "\n",
    "        # input image dimensions\n",
    "        img_rows, img_cols = 28, 28\n",
    "\n",
    "        # the data, split between train and test sets\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "            x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "            input_shape = (1, img_rows, img_cols)\n",
    "        else:\n",
    "            x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "            x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "            input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "#             print('x_train shape:', x_train.shape)\n",
    "#             print(x_train.shape[0], 'train samples')\n",
    "#             print(x_test.shape[0], 'test samples')\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "        y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "            ########\n",
    "            \n",
    "            \n",
    "\n",
    "        for gen in element:\n",
    "            if (gen== 0):\n",
    "                model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "            elif (gen == 1):\n",
    "                model.add(Dense(128, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                          optimizer=keras.optimizers.Adadelta(),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "        model.fit(x_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      verbose=1,\n",
    "                      validation_data=(x_test, y_test))\n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "        accuracy_of_population.append(score[1])\n",
    "        \n",
    "    return (accuracy_of_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# umschreiben ist nur der erste durchlauf\n",
    "\n",
    "def netzdurchlauf1(acutalpop):\n",
    "    accuracy_of_population= []\n",
    "    \n",
    "    print('Aktuelle Population')\n",
    "    print(actualpop)\n",
    "    for element in actualpop:\n",
    "        for indiv in element:\n",
    "            batch_size = 128\n",
    "            num_classes = 10\n",
    "            epochs = 12\n",
    "\n",
    "            # input image dimensions\n",
    "            img_rows, img_cols = 28, 28\n",
    "\n",
    "            # the data, split between train and test sets\n",
    "            (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "            if K.image_data_format() == 'channels_first':\n",
    "                x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "                x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "                input_shape = (1, img_rows, img_cols)\n",
    "            else:\n",
    "                x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "                x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "                input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "            x_train = x_train.astype('float32')\n",
    "            x_test = x_test.astype('float32')\n",
    "            x_train /= 255\n",
    "            x_test /= 255\n",
    "#             print('x_train shape:', x_train.shape)\n",
    "#             print(x_train.shape[0], 'train samples')\n",
    "#             print(x_test.shape[0], 'test samples')\n",
    "\n",
    "            # convert class vectors to binary class matrices\n",
    "            y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "            y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "            model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                             activation='relu',\n",
    "                             input_shape=input_shape))\n",
    "            model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            model.add(Dropout(0.25))\n",
    "            ########\n",
    "            \n",
    "            \n",
    "\n",
    "            for gen in indiv:\n",
    "                if (gen== 0):\n",
    "                    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "                elif (gen == 1):\n",
    "                    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            model.add(Flatten())\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "            model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                          optimizer=keras.optimizers.Adadelta(),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            model.fit(x_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      verbose=1,\n",
    "                      validation_data=(x_test, y_test))\n",
    "            score = model.evaluate(x_test, y_test, verbose=0)\n",
    "            print('Test loss:', score[0])\n",
    "            print('Test accuracy:', score[1])\n",
    "            accuracy_of_population.append(score[1])\n",
    "        \n",
    "    return (accuracy_of_population)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aktuelle Population\n",
      "[[array([0, 1, 0, 0, 0, 1])], [array([1, 1, 1, 1, 1, 0])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0914 16:52:32.434784 16700 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0914 16:52:32.449743 16700 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0914 16:52:32.451693 16700 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0914 16:52:32.487595 16700 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0914 16:52:32.490589 16700 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0914 16:52:32.502557 16700 deprecation.py:506] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0914 16:52:32.653185 16700 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0914 16:52:32.660136 16700 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0914 16:52:32.746487 16700 deprecation.py:323] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 216us/step - loss: 0.3659 - acc: 0.8806 - val_loss: 0.1257 - val_acc: 0.9577\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.0762 - acc: 0.9771 - val_loss: 0.0465 - val_acc: 0.9851\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 9s 157us/step - loss: 0.0506 - acc: 0.9847 - val_loss: 0.0353 - val_acc: 0.9891\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0404 - acc: 0.9873 - val_loss: 0.0249 - val_acc: 0.9916\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0316 - acc: 0.9903 - val_loss: 0.0259 - val_acc: 0.9913\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.0267 - acc: 0.9921 - val_loss: 0.0221 - val_acc: 0.9933\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.0243 - acc: 0.9930 - val_loss: 0.0229 - val_acc: 0.9935\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.0217 - acc: 0.9929 - val_loss: 0.0236 - val_acc: 0.9926\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.0179 - acc: 0.9945 - val_loss: 0.0195 - val_acc: 0.9940s - loss: 0.0178 - acc\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.0147 - acc: 0.9953 - val_loss: 0.0255 - val_acc: 0.9935\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0143 - acc: 0.9954 - val_loss: 0.0204 - val_acc: 0.9940\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.0127 - acc: 0.9960 - val_loss: 0.0223 - val_acc: 0.9945\n",
      "Test loss: 0.0222583491799327\n",
      "Test accuracy: 0.9945\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 14s 241us/step - loss: 0.2728 - acc: 0.9155 - val_loss: 0.0738 - val_acc: 0.9797\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 14s 234us/step - loss: 0.0913 - acc: 0.9722 - val_loss: 0.0530 - val_acc: 0.9824\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0690 - acc: 0.9790 - val_loss: 0.0413 - val_acc: 0.9859\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0579 - acc: 0.9823 - val_loss: 0.0508 - val_acc: 0.9833\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.0498 - acc: 0.9853 - val_loss: 0.0363 - val_acc: 0.9874\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.0463 - acc: 0.9861 - val_loss: 0.0326 - val_acc: 0.9887\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0407 - acc: 0.9872 - val_loss: 0.0318 - val_acc: 0.9891\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.0388 - acc: 0.9880 - val_loss: 0.0316 - val_acc: 0.9894\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.0364 - acc: 0.9889 - val_loss: 0.0287 - val_acc: 0.9907\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 14s 234us/step - loss: 0.0345 - acc: 0.9893 - val_loss: 0.0267 - val_acc: 0.9908\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.0325 - acc: 0.9897 - val_loss: 0.0313 - val_acc: 0.9896\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 13s 214us/step - loss: 0.0289 - acc: 0.9911 - val_loss: 0.0298 - val_acc: 0.9899\n",
      "Test loss: 0.029816957558694412\n",
      "Test accuracy: 0.9899\n",
      " Accuracy of first population\n",
      "[0.9945, 0.9899]\n",
      "highest accuracy of first population:\n",
      "0.9945\n",
      "Highest acc at\n",
      "[0 1]\n",
      "Aktuelle Population\n",
      "[array([1, 1, 1, 0, 0, 1]), array([0, 1, 0, 1, 1, 0])]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 209us/step - loss: 0.2626 - acc: 0.9173 - val_loss: 0.0614 - val_acc: 0.9804\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 0.0720 - acc: 0.9782 - val_loss: 0.0422 - val_acc: 0.9872\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 0.0522 - acc: 0.9838 - val_loss: 0.0529 - val_acc: 0.9822\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 0.0418 - acc: 0.9874 - val_loss: 0.0281 - val_acc: 0.9902\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 0.0368 - acc: 0.9888 - val_loss: 0.0299 - val_acc: 0.9913\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 12s 205us/step - loss: 0.0325 - acc: 0.9901 - val_loss: 0.0260 - val_acc: 0.9922\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.0281 - acc: 0.9913 - val_loss: 0.0275 - val_acc: 0.9914\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 0.0252 - acc: 0.9922 - val_loss: 0.0249 - val_acc: 0.9918\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 12s 207us/step - loss: 0.0237 - acc: 0.9929 - val_loss: 0.0276 - val_acc: 0.9912\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 0.0204 - acc: 0.9939 - val_loss: 0.0227 - val_acc: 0.9924\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.0187 - acc: 0.9941 - val_loss: 0.0238 - val_acc: 0.9925\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.0176 - acc: 0.9947 - val_loss: 0.0291 - val_acc: 0.9917\n",
      "Test loss: 0.029104717080222327\n",
      "Test accuracy: 0.9917\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.3477 - acc: 0.8883 - val_loss: 0.0693 - val_acc: 0.9802\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0849 - acc: 0.9738 - val_loss: 0.0413 - val_acc: 0.9861\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0585 - acc: 0.9818 - val_loss: 0.0366 - val_acc: 0.9883\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: 0.0459 - acc: 0.9859 - val_loss: 0.0247 - val_acc: 0.9917\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.0384 - acc: 0.9883 - val_loss: 0.0271 - val_acc: 0.9916\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.0341 - acc: 0.9895 - val_loss: 0.0253 - val_acc: 0.9921\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0297 - acc: 0.9906 - val_loss: 0.0256 - val_acc: 0.9919\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0268 - acc: 0.9912 - val_loss: 0.0234 - val_acc: 0.9930\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0235 - acc: 0.9926 - val_loss: 0.0234 - val_acc: 0.9929\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0221 - acc: 0.9929 - val_loss: 0.0197 - val_acc: 0.9934\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.0200 - acc: 0.9937 - val_loss: 0.0202 - val_acc: 0.9943\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.0208 - acc: 0.9936 - val_loss: 0.0239 - val_acc: 0.9930\n",
      "Test loss: 0.023865820949354383\n",
      "Test accuracy: 0.993\n",
      " Accuracy of populationnummer: \n",
      "1\n",
      "[0.9917, 0.993]\n",
      "highest accuracy of populationnummer: \n",
      "1\n",
      "0.993\n",
      "[array([1, 1, 1, 0, 0, 1]), array([0, 1, 0, 1, 1, 0])]\n",
      "Highest acc at\n",
      "[1 0]\n",
      "Mutation\n",
      "Aktuelle Population\n",
      "[array([1, 1, 1, 1, 1, 1]), array([0, 1, 0, 0, 0, 1])]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 14s 233us/step - loss: 0.4212 - acc: 0.8732 - val_loss: 0.2154 - val_acc: 0.9379\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 13s 219us/step - loss: 0.1752 - acc: 0.9488 - val_loss: 0.1081 - val_acc: 0.9681\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.1018 - acc: 0.9691 - val_loss: 0.0778 - val_acc: 0.9747\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.0788 - acc: 0.9756 - val_loss: 0.0614 - val_acc: 0.9803\n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 14s 241us/step - loss: 0.0660 - acc: 0.9796 - val_loss: 0.0635 - val_acc: 0.9792\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 14s 242us/step - loss: 0.0603 - acc: 0.9816 - val_loss: 0.0485 - val_acc: 0.9851\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0533 - acc: 0.9830 - val_loss: 0.0445 - val_acc: 0.9849\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0490 - acc: 0.9845 - val_loss: 0.0518 - val_acc: 0.9833\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0438 - acc: 0.9863 - val_loss: 0.0433 - val_acc: 0.9859\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.0411 - acc: 0.9870 - val_loss: 0.0377 - val_acc: 0.9879\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.0390 - acc: 0.9880 - val_loss: 0.0408 - val_acc: 0.9868\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0364 - acc: 0.9890 - val_loss: 0.0471 - val_acc: 0.9857\n",
      "Test loss: 0.04711078703081002\n",
      "Test accuracy: 0.9857\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.3564 - acc: 0.8848 - val_loss: 0.0907 - val_acc: 0.9717\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.0744 - acc: 0.9775 - val_loss: 0.0452 - val_acc: 0.9868\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0510 - acc: 0.9848 - val_loss: 0.0264 - val_acc: 0.9915\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0376 - acc: 0.9886 - val_loss: 0.0214 - val_acc: 0.9932\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0322 - acc: 0.9901 - val_loss: 0.0232 - val_acc: 0.9927\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 10s 171us/step - loss: 0.0260 - acc: 0.9921 - val_loss: 0.0227 - val_acc: 0.9926\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.0226 - acc: 0.9930 - val_loss: 0.0219 - val_acc: 0.9932\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.0200 - acc: 0.9937 - val_loss: 0.0357 - val_acc: 0.9886\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0178 - acc: 0.9948 - val_loss: 0.0229 - val_acc: 0.9926\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.0158 - acc: 0.9947 - val_loss: 0.0203 - val_acc: 0.9934\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0143 - acc: 0.9956 - val_loss: 0.0184 - val_acc: 0.9954\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.0129 - acc: 0.9959 - val_loss: 0.0250 - val_acc: 0.9933\n",
      "Test loss: 0.025049934124757964\n",
      "Test accuracy: 0.9933\n",
      " Accuracy of populationnummer: \n",
      "2\n",
      "[0.9857, 0.9933]\n",
      "highest accuracy of populationnummer: \n",
      "2\n",
      "0.9933\n",
      "[array([1, 1, 1, 1, 1, 1]), array([0, 1, 0, 0, 0, 1])]\n",
      "Highest acc at\n",
      "[1 0]\n",
      "Aktuelle Population\n",
      "[array([1, 1, 1, 0, 0, 1]), array([0, 1, 0, 1, 1, 1])]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.2603 - acc: 0.9193 - val_loss: 0.0506 - val_acc: 0.9827\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 0.0711 - acc: 0.9785 - val_loss: 0.0375 - val_acc: 0.9872\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.0511 - acc: 0.9844 - val_loss: 0.0378 - val_acc: 0.9875\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 12s 205us/step - loss: 0.0411 - acc: 0.9877 - val_loss: 0.0351 - val_acc: 0.9895\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 13s 218us/step - loss: 0.0340 - acc: 0.9897 - val_loss: 0.0291 - val_acc: 0.9892\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0311 - acc: 0.9901 - val_loss: 0.0300 - val_acc: 0.9891\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0275 - acc: 0.9915 - val_loss: 0.0268 - val_acc: 0.9907\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0241 - acc: 0.9920 - val_loss: 0.0245 - val_acc: 0.9914\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.0221 - acc: 0.9927 - val_loss: 0.0241 - val_acc: 0.9925\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.0189 - acc: 0.9939 - val_loss: 0.0245 - val_acc: 0.9916\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 0.0174 - acc: 0.9944 - val_loss: 0.0248 - val_acc: 0.9923\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 12s 205us/step - loss: 0.0162 - acc: 0.9948 - val_loss: 0.0213 - val_acc: 0.9926\n",
      "Test loss: 0.02134368190515379\n",
      "Test accuracy: 0.9926\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.2752 - acc: 0.9137 - val_loss: 0.0664 - val_acc: 0.9806\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0748 - acc: 0.9774 - val_loss: 0.0521 - val_acc: 0.9837\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0526 - acc: 0.9841 - val_loss: 0.0338 - val_acc: 0.9890\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.0423 - acc: 0.9868 - val_loss: 0.0295 - val_acc: 0.9907\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0359 - acc: 0.9893 - val_loss: 0.0282 - val_acc: 0.9901\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0308 - acc: 0.9902 - val_loss: 0.0393 - val_acc: 0.9868\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0267 - acc: 0.9917 - val_loss: 0.0260 - val_acc: 0.9920\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0248 - acc: 0.9921 - val_loss: 0.0250 - val_acc: 0.9918\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0213 - acc: 0.9931 - val_loss: 0.0220 - val_acc: 0.9926\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0190 - acc: 0.9941 - val_loss: 0.0277 - val_acc: 0.9927\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.0183 - acc: 0.9942 - val_loss: 0.0232 - val_acc: 0.9927\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0156 - acc: 0.9951 - val_loss: 0.0247 - val_acc: 0.9922\n",
      "Test loss: 0.02473313745137275\n",
      "Test accuracy: 0.9922\n",
      " Accuracy of populationnummer: \n",
      "3\n",
      "[0.9926, 0.9922]\n",
      "highest accuracy of populationnummer: \n",
      "3\n",
      "0.9926\n",
      "[array([1, 1, 1, 0, 0, 1]), array([0, 1, 0, 1, 1, 1])]\n",
      "Highest acc at\n",
      "[0 1]\n",
      "Aktuelle Population\n",
      "[array([0, 1, 0, 0, 0, 1]), array([1, 1, 1, 1, 1, 1])]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.3586 - acc: 0.8831 - val_loss: 0.0720 - val_acc: 0.9761\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.0733 - acc: 0.9774 - val_loss: 0.0407 - val_acc: 0.9877\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 10s 159us/step - loss: 0.0471 - acc: 0.9860 - val_loss: 0.0335 - val_acc: 0.9890\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.0380 - acc: 0.9882 - val_loss: 0.0441 - val_acc: 0.9864\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.0301 - acc: 0.9904 - val_loss: 0.0268 - val_acc: 0.9912\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.0260 - acc: 0.9918 - val_loss: 0.0225 - val_acc: 0.9926\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.0217 - acc: 0.9927 - val_loss: 0.0209 - val_acc: 0.9935\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 10s 165us/step - loss: 0.0187 - acc: 0.9944 - val_loss: 0.0195 - val_acc: 0.9935\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.0171 - acc: 0.9946 - val_loss: 0.0340 - val_acc: 0.9904\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.0143 - acc: 0.9953 - val_loss: 0.0227 - val_acc: 0.9928\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.0136 - acc: 0.9957 - val_loss: 0.0233 - val_acc: 0.9928\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.0121 - acc: 0.9961 - val_loss: 0.0246 - val_acc: 0.9934\n",
      "Test loss: 0.024624105880165916\n",
      "Test accuracy: 0.9934\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 0.4102 - acc: 0.8749 - val_loss: 0.1506 - val_acc: 0.9562\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.1368 - acc: 0.9589 - val_loss: 0.0745 - val_acc: 0.9777\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.0938 - acc: 0.9713 - val_loss: 0.0628 - val_acc: 0.9807\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 14s 233us/step - loss: 0.0756 - acc: 0.9776 - val_loss: 0.0761 - val_acc: 0.9764\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 15s 245us/step - loss: 0.0630 - acc: 0.9809 - val_loss: 0.0478 - val_acc: 0.9848\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 0.0565 - acc: 0.9826 - val_loss: 0.0452 - val_acc: 0.9861\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 0.0502 - acc: 0.9848 - val_loss: 0.0520 - val_acc: 0.9838\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.0460 - acc: 0.9857 - val_loss: 0.0437 - val_acc: 0.9855\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.0416 - acc: 0.9872 - val_loss: 0.0399 - val_acc: 0.9874\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 14s 233us/step - loss: 0.0383 - acc: 0.9883 - val_loss: 0.0369 - val_acc: 0.9874\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.0355 - acc: 0.9885 - val_loss: 0.0355 - val_acc: 0.9884\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 15s 247us/step - loss: 0.0342 - acc: 0.9895 - val_loss: 0.0343 - val_acc: 0.9882\n",
      "Test loss: 0.03429785780865932\n",
      "Test accuracy: 0.9882\n",
      " Accuracy of populationnummer: \n",
      "4\n",
      "[0.9934, 0.9882]\n",
      "highest accuracy of populationnummer: \n",
      "4\n",
      "0.9934\n",
      "[array([0, 1, 0, 0, 0, 1]), array([1, 1, 1, 1, 1, 1])]\n",
      "Highest acc at\n",
      "[0 1]\n",
      "Aktuelle Population\n",
      "[array([1, 1, 1, 0, 0, 1]), array([0, 1, 0, 1, 1, 1])]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 14s 242us/step - loss: 0.2770 - acc: 0.9122 - val_loss: 0.0700 - val_acc: 0.9771\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 0.0715 - acc: 0.9779 - val_loss: 0.0534 - val_acc: 0.9859\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.0525 - acc: 0.9835 - val_loss: 0.0353 - val_acc: 0.9888\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 0.0424 - acc: 0.9875 - val_loss: 0.0281 - val_acc: 0.9902\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 0.0371 - acc: 0.9882 - val_loss: 0.0272 - val_acc: 0.9910\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 0.0317 - acc: 0.9905 - val_loss: 0.0301 - val_acc: 0.9900\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 0.0291 - acc: 0.9906 - val_loss: 0.0228 - val_acc: 0.9929\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.0254 - acc: 0.9920 - val_loss: 0.0246 - val_acc: 0.9917\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.0226 - acc: 0.9929 - val_loss: 0.0255 - val_acc: 0.9923\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 13s 219us/step - loss: 0.0203 - acc: 0.9932 - val_loss: 0.0242 - val_acc: 0.9916\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0187 - acc: 0.9939 - val_loss: 0.0233 - val_acc: 0.9931\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 0.0169 - acc: 0.9946 - val_loss: 0.0207 - val_acc: 0.9936\n",
      "Test loss: 0.020739602913456837\n",
      "Test accuracy: 0.9936\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 212us/step - loss: 0.2940 - acc: 0.9056 - val_loss: 0.0691 - val_acc: 0.9786\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0733 - acc: 0.9778 - val_loss: 0.0398 - val_acc: 0.9877\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0523 - acc: 0.9839 - val_loss: 0.0470 - val_acc: 0.9845\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0423 - acc: 0.9874 - val_loss: 0.0301 - val_acc: 0.9905\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0364 - acc: 0.9887 - val_loss: 0.0256 - val_acc: 0.9912\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0314 - acc: 0.9906 - val_loss: 0.0296 - val_acc: 0.9898\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 12s 195us/step - loss: 0.0276 - acc: 0.9916 - val_loss: 0.0277 - val_acc: 0.9903\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 0.0256 - acc: 0.9923 - val_loss: 0.0240 - val_acc: 0.9926\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0216 - acc: 0.9931 - val_loss: 0.0271 - val_acc: 0.9916\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0199 - acc: 0.9939 - val_loss: 0.0225 - val_acc: 0.9931\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0180 - acc: 0.9947 - val_loss: 0.0248 - val_acc: 0.9925\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.0165 - acc: 0.9945 - val_loss: 0.0256 - val_acc: 0.9927\n",
      "Test loss: 0.025600703929841984\n",
      "Test accuracy: 0.9927\n",
      " Accuracy of populationnummer: \n",
      "5\n",
      "[0.9936, 0.9927]\n",
      "highest accuracy of populationnummer: \n",
      "5\n",
      "0.9936\n",
      "[array([1, 1, 1, 0, 0, 1]), array([0, 1, 0, 1, 1, 1])]\n",
      "Highest acc at\n",
      "[0 1]\n",
      "Aktuelle Population\n",
      "[array([0, 1, 0, 0, 0, 1]), array([1, 1, 1, 1, 1, 1])]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.3403 - acc: 0.8888 - val_loss: 0.1403 - val_acc: 0.9569\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.0732 - acc: 0.9781 - val_loss: 0.0442 - val_acc: 0.9855\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0502 - acc: 0.9848 - val_loss: 0.0303 - val_acc: 0.9908\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0389 - acc: 0.9884 - val_loss: 0.0267 - val_acc: 0.9916\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0324 - acc: 0.9896 - val_loss: 0.0240 - val_acc: 0.9920\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.0275 - acc: 0.9914 - val_loss: 0.0203 - val_acc: 0.9932\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 10s 161us/step - loss: 0.0240 - acc: 0.9928 - val_loss: 0.0296 - val_acc: 0.9916\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 10s 169us/step - loss: 0.0195 - acc: 0.9939 - val_loss: 0.0221 - val_acc: 0.9937\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0179 - acc: 0.9942 - val_loss: 0.0200 - val_acc: 0.9941\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0158 - acc: 0.9954 - val_loss: 0.0220 - val_acc: 0.9931\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.0144 - acc: 0.9953 - val_loss: 0.0259 - val_acc: 0.9929\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0124 - acc: 0.9958 - val_loss: 0.0208 - val_acc: 0.9941\n",
      "Test loss: 0.020791257555023913\n",
      "Test accuracy: 0.9941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 16s 262us/step - loss: 0.3844 - acc: 0.8839 - val_loss: 0.1066 - val_acc: 0.9684\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 14s 236us/step - loss: 0.1183 - acc: 0.9642 - val_loss: 0.0729 - val_acc: 0.9772\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 14s 238us/step - loss: 0.0896 - acc: 0.9724 - val_loss: 0.0589 - val_acc: 0.9818\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 15s 248us/step - loss: 0.0757 - acc: 0.9767 - val_loss: 0.0594 - val_acc: 0.9814\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 0.0665 - acc: 0.9797 - val_loss: 0.0873 - val_acc: 0.9759\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 15s 245us/step - loss: 0.0616 - acc: 0.9809 - val_loss: 0.0465 - val_acc: 0.9846\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.0558 - acc: 0.9829 - val_loss: 0.0435 - val_acc: 0.9869\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 14s 234us/step - loss: 0.0518 - acc: 0.9834 - val_loss: 0.0625 - val_acc: 0.9795\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 14s 236us/step - loss: 0.0483 - acc: 0.9852 - val_loss: 0.0720 - val_acc: 0.9765\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.0448 - acc: 0.9865 - val_loss: 0.0444 - val_acc: 0.9857\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 14s 236us/step - loss: 0.0427 - acc: 0.9871 - val_loss: 0.0470 - val_acc: 0.9857\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 14s 237us/step - loss: 0.0396 - acc: 0.9874 - val_loss: 0.0422 - val_acc: 0.9861\n",
      "Test loss: 0.042233877308666706\n",
      "Test accuracy: 0.9861\n",
      " Accuracy of populationnummer: \n",
      "6\n",
      "[0.9941, 0.9861]\n",
      "highest accuracy of populationnummer: \n",
      "6\n",
      "0.9941\n",
      "[array([0, 1, 0, 0, 0, 1]), array([1, 1, 1, 1, 1, 1])]\n",
      "Highest acc at\n",
      "[0 1]\n",
      "Mutation\n",
      "Aktuelle Population\n",
      "[array([0, 1, 0, 1, 1, 1]), array([0, 1, 1, 0, 0, 1])]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.2987 - acc: 0.9072 - val_loss: 0.0642 - val_acc: 0.9803\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.0748 - acc: 0.9772 - val_loss: 0.0390 - val_acc: 0.9877\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0538 - acc: 0.9831 - val_loss: 0.0410 - val_acc: 0.9870\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 192us/step - loss: 0.0434 - acc: 0.9867 - val_loss: 0.0278 - val_acc: 0.9909\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 12s 195us/step - loss: 0.0368 - acc: 0.9887 - val_loss: 0.0304 - val_acc: 0.9899\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.0313 - acc: 0.9906 - val_loss: 0.0271 - val_acc: 0.9908\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.0275 - acc: 0.9913 - val_loss: 0.0298 - val_acc: 0.9898\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0266 - acc: 0.9917 - val_loss: 0.0255 - val_acc: 0.9906\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: 0.0232 - acc: 0.9927 - val_loss: 0.0245 - val_acc: 0.9916\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 0.0211 - acc: 0.9933 - val_loss: 0.0201 - val_acc: 0.9932\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 0.0190 - acc: 0.9942 - val_loss: 0.0231 - val_acc: 0.9928\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 0.0180 - acc: 0.9944 - val_loss: 0.0206 - val_acc: 0.9932\n",
      "Test loss: 0.020571259874053067\n",
      "Test accuracy: 0.9932\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.2900 - acc: 0.9076 - val_loss: 0.0551 - val_acc: 0.9821\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 10s 172us/step - loss: 0.0710 - acc: 0.9787 - val_loss: 0.0522 - val_acc: 0.9836\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0497 - acc: 0.9850 - val_loss: 0.0361 - val_acc: 0.9893\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.0388 - acc: 0.9881 - val_loss: 0.0263 - val_acc: 0.9921\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.0341 - acc: 0.9894 - val_loss: 0.0281 - val_acc: 0.9904\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.0293 - acc: 0.9906 - val_loss: 0.0373 - val_acc: 0.9893\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.0251 - acc: 0.9920 - val_loss: 0.0316 - val_acc: 0.9900\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.0231 - acc: 0.9927 - val_loss: 0.0230 - val_acc: 0.9930\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0206 - acc: 0.9935 - val_loss: 0.0217 - val_acc: 0.9927: 0.9\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0185 - acc: 0.9941 - val_loss: 0.0234 - val_acc: 0.9933\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.0172 - acc: 0.9942 - val_loss: 0.0198 - val_acc: 0.9937\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0152 - acc: 0.9953 - val_loss: 0.0233 - val_acc: 0.9932\n",
      "Test loss: 0.023253342989923296\n",
      "Test accuracy: 0.9932\n",
      " Accuracy of populationnummer: \n",
      "7\n",
      "[0.9932, 0.9932]\n",
      "highest accuracy of populationnummer: \n",
      "7\n",
      "0.9932\n",
      "[array([0, 1, 0, 1, 1, 1]), array([0, 1, 1, 0, 0, 1])]\n",
      "Highest acc at\n",
      "[1 0]\n",
      "Mutation\n",
      "Aktuelle Population\n",
      "[array([0, 1, 0, 0, 0, 1]), array([0, 1, 1, 0, 1, 1])]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 12s 207us/step - loss: 0.3545 - acc: 0.8838 - val_loss: 0.0565 - val_acc: 0.9823\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: 0.0712 - acc: 0.9781 - val_loss: 0.0339 - val_acc: 0.9898\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0499 - acc: 0.9845 - val_loss: 0.0289 - val_acc: 0.9914\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.0370 - acc: 0.9889 - val_loss: 0.0324 - val_acc: 0.9907\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 175us/step - loss: 0.0302 - acc: 0.9902 - val_loss: 0.0215 - val_acc: 0.9925\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 10s 166us/step - loss: 0.0262 - acc: 0.9920 - val_loss: 0.0193 - val_acc: 0.9938\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 10s 170us/step - loss: 0.0225 - acc: 0.9934 - val_loss: 0.0275 - val_acc: 0.9913\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 10s 167us/step - loss: 0.0203 - acc: 0.9933 - val_loss: 0.0414 - val_acc: 0.9873\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.0178 - acc: 0.9944 - val_loss: 0.0219 - val_acc: 0.9938\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.0153 - acc: 0.9953 - val_loss: 0.0220 - val_acc: 0.9938\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0127 - acc: 0.9962 - val_loss: 0.0284 - val_acc: 0.9917\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0118 - acc: 0.9964 - val_loss: 0.0214 - val_acc: 0.9936\n",
      "Test loss: 0.021359273959394524\n",
      "Test accuracy: 0.9936\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.2643 - acc: 0.9168 - val_loss: 0.0652 - val_acc: 0.9793\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0765 - acc: 0.9766 - val_loss: 0.0415 - val_acc: 0.9861\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 192us/step - loss: 0.0568 - acc: 0.9825 - val_loss: 0.0366 - val_acc: 0.9872\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.0454 - acc: 0.9867 - val_loss: 0.0312 - val_acc: 0.9890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.0376 - acc: 0.9882 - val_loss: 0.0292 - val_acc: 0.9899\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 13s 209us/step - loss: 0.0329 - acc: 0.9898 - val_loss: 0.0312 - val_acc: 0.9898\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 12s 207us/step - loss: 0.0292 - acc: 0.9913 - val_loss: 0.0259 - val_acc: 0.9916\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 0.0258 - acc: 0.9921 - val_loss: 0.0250 - val_acc: 0.9915\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0238 - acc: 0.9926 - val_loss: 0.0252 - val_acc: 0.9917\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 11s 192us/step - loss: 0.0211 - acc: 0.9934 - val_loss: 0.0235 - val_acc: 0.9921\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0190 - acc: 0.9940 - val_loss: 0.0234 - val_acc: 0.9918\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0177 - acc: 0.9944 - val_loss: 0.0227 - val_acc: 0.9931\n",
      "Test loss: 0.02274537491970259\n",
      "Test accuracy: 0.9931\n",
      " Accuracy of populationnummer: \n",
      "8\n",
      "[0.9936, 0.9931]\n",
      "highest accuracy of populationnummer: \n",
      "8\n",
      "0.9936\n",
      "[array([0, 1, 0, 0, 0, 1]), array([0, 1, 1, 0, 1, 1])]\n",
      "Highest acc at\n",
      "[0 1]\n",
      "Aktuelle Population\n",
      "[array([0, 1, 1, 0, 0, 1]), array([0, 1, 0, 0, 1, 1])]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 0.3002 - acc: 0.9051 - val_loss: 0.0818 - val_acc: 0.9751\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.0692 - acc: 0.9786 - val_loss: 0.0372 - val_acc: 0.9888\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0488 - acc: 0.9849 - val_loss: 0.0337 - val_acc: 0.9896\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.0378 - acc: 0.9881 - val_loss: 0.0233 - val_acc: 0.9925\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 0.0316 - acc: 0.9900 - val_loss: 0.0202 - val_acc: 0.9934\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.0276 - acc: 0.9910 - val_loss: 0.0257 - val_acc: 0.9919\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.0228 - acc: 0.9929 - val_loss: 0.0249 - val_acc: 0.9932\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0214 - acc: 0.9935 - val_loss: 0.0275 - val_acc: 0.9920\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0207 - acc: 0.9930 - val_loss: 0.0190 - val_acc: 0.9933\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.0176 - acc: 0.9947 - val_loss: 0.0212 - val_acc: 0.9943\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 0.0162 - acc: 0.9949 - val_loss: 0.0204 - val_acc: 0.9943\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.0140 - acc: 0.9954 - val_loss: 0.0204 - val_acc: 0.9937\n",
      "Test loss: 0.020392296344032276\n",
      "Test accuracy: 0.9937\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 218us/step - loss: 0.2961 - acc: 0.9067 - val_loss: 0.0528 - val_acc: 0.9827\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0709 - acc: 0.9789 - val_loss: 0.0362 - val_acc: 0.9878\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0488 - acc: 0.9849 - val_loss: 0.0262 - val_acc: 0.9909\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.0380 - acc: 0.9884 - val_loss: 0.0287 - val_acc: 0.9914\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.0327 - acc: 0.9898 - val_loss: 0.0248 - val_acc: 0.9920\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0281 - acc: 0.9909 - val_loss: 0.0260 - val_acc: 0.9918\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0238 - acc: 0.9921 - val_loss: 0.0236 - val_acc: 0.9932\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0218 - acc: 0.9933 - val_loss: 0.0298 - val_acc: 0.9906\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0191 - acc: 0.9940 - val_loss: 0.0212 - val_acc: 0.9931\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 10s 174us/step - loss: 0.0169 - acc: 0.9945 - val_loss: 0.0204 - val_acc: 0.9940\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0156 - acc: 0.9951 - val_loss: 0.0216 - val_acc: 0.9935\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 177us/step - loss: 0.0136 - acc: 0.9957 - val_loss: 0.0258 - val_acc: 0.9931\n",
      "Test loss: 0.02582888941942847\n",
      "Test accuracy: 0.9931\n",
      " Accuracy of populationnummer: \n",
      "9\n",
      "[0.9937, 0.9931]\n",
      "highest accuracy of populationnummer: \n",
      "9\n",
      "0.9937\n",
      "[array([0, 1, 1, 0, 0, 1]), array([0, 1, 0, 0, 1, 1])]\n",
      "Highest acc at\n",
      "[0 1]\n",
      "The highest accuracies in population\n",
      "[[1, 0.9945], [2, 0.993], [3, 0.9933], [4, 0.9926], [5, 0.9934], [6, 0.9936], [7, 0.9941], [8, 0.9932], [9, 0.9936], [10, 0.9937]]\n",
      "Wartezeit\n",
      "0.002190839000006939\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hauptmethode\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "DURCHGNGE = 10 # variable to clarify number of generations\n",
    "actualpop= initialise()\n",
    "evalaccuris = []\n",
    "saved = actualpop\n",
    "i = 0\n",
    "if i == 0:\n",
    "    accuri = netzdurchlauf1(actualpop)\n",
    "    print(' Accuracy of first population')\n",
    "    print(accuri)\n",
    "    print('highest accuracy of first population:')\n",
    "    print(np.amax(accuri))\n",
    "    actualpop = selected(accuri, actualpop)\n",
    "    actualpop = mutation(actualpop)\n",
    "    evalaccuris.append([i +1 ,np.amax(accuri)])\n",
    "    i= i+1\n",
    "    \n",
    "    \n",
    "while i < DURCHGNGE:\n",
    "    \n",
    "    nummer = i\n",
    "    accuri = netzdurchlauf(actualpop)\n",
    "    print(' Accuracy of populationnummer: ')\n",
    "    print( nummer)\n",
    "    print(accuri)\n",
    "    print('highest accuracy of populationnummer: ')\n",
    "    print( nummer)\n",
    "    print(np.amax(accuri))\n",
    "    print(actualpop)\n",
    "    actualpop = selected2(accuri, actualpop)\n",
    "    actualpop = mutation(actualpop)\n",
    "    \n",
    "    evalaccuris.append([i +1 ,np.amax(accuri)])\n",
    "    i = i+1\n",
    "    \n",
    "    \n",
    "    \n",
    "print('The highest accuracies in population')\n",
    "print(evalaccuris)  \n",
    "\n",
    "print('Wartezeit')\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.99, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(data =evalaccuris)\n",
    "df.rename(columns={0: 'Generation',1:'Accuracy'}, inplace=True)\n",
    "sns.regplot(x=df['Generation'],y=df['Accuracy'])\n",
    "plt.ylim(0.990, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
