{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageDraw\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random start of population\n",
    "#genomes as a list of gene sequences\n",
    "def initialise():\n",
    "    genomes =[]\n",
    "    for i in range(8):\n",
    "        genomes.append([np.random.randint(2, size=6)]) # add 5 randomly initialised indiviudals\n",
    "    \n",
    "    #entschachtelungsprozess\n",
    "    ent = [5]*len(genomes)\n",
    "    i = 0\n",
    "    for k in genomes:\n",
    "        for l in k:\n",
    "            \n",
    "            ent[i] =l\n",
    "            i = i+1\n",
    "    \n",
    "\n",
    "    return ent\n",
    "\n",
    "def craschcheck(popul):\n",
    "    i=0\n",
    "    for indi in popul:\n",
    "        if check(indi) == True:\n",
    "            continue;\n",
    "        if check(indi) == False:\n",
    "            indi[2] = 1\n",
    "            print('Problem')\n",
    "            popul[i] = np.array(indi)\n",
    "        \n",
    "            \n",
    "        i = i +1\n",
    "                \n",
    "\n",
    "            \n",
    "def check(indi):\n",
    "    for gen in indi:\n",
    "        if gen == 1:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usefull functions\n",
    "\n",
    "def paring(mom,dad):\n",
    "    return(np.concatenate((mom[:3], dad[3:]), axis=0))\n",
    "\n",
    "def entschachteln(genos):\n",
    "    #print(len(genos))\n",
    "    neue = [0,0]\n",
    "    i = 0\n",
    "    for k in genos:\n",
    "        for l in k:\n",
    "            #print(l)\n",
    "            neue[i] =l\n",
    "            i = i+1\n",
    "\n",
    "    return (neue)\n",
    "\n",
    "def mutation(popul):\n",
    "    position = random.randrange(0, len(popul))\n",
    "    gencode = popul[position]\n",
    "    \n",
    "    decider = random.uniform(0, 1)\n",
    "    posi = 0\n",
    "    if decider < 0.8 :\n",
    "       \n",
    "        print('Mutation')\n",
    "        posi = random.randrange(0, len(gencode))\n",
    "    \n",
    "        if gencode[posi] == 1:\n",
    "            gencode[posi] = 0\n",
    "        elif gencode[posi] == 0:\n",
    "            gencode[posi] = 1\n",
    "            \n",
    "    popul[position] = np.array(gencode)\n",
    "    \n",
    "    return popul\n",
    "\n",
    "def mutationeach(popul):\n",
    "  \n",
    "    i = 0    \n",
    "    for gencode in popul:\n",
    "        decider = random.uniform(0, 1)\n",
    "        if decider < 0.351: \n",
    "            print('Muation')\n",
    "            posi = 0\n",
    "            posi = random.randrange(0, len(gencode))\n",
    "           \n",
    "            if gencode[posi] == 1:\n",
    "                gencode[posi] = 0\n",
    "            elif gencode[posi] == 0:\n",
    "                gencode[posi] = 1\n",
    "            \n",
    "            popul[i] = np.array(gencode)\n",
    "        \n",
    "            \n",
    "        i = i +1\n",
    "    \n",
    "\n",
    "            \n",
    "    \n",
    "    return popul\n",
    "    \n",
    "\n",
    "''' A one to one replacement reproduction '''\n",
    "def selected(maxis, actualpop): \n",
    "    childs= []\n",
    "    maxis= np.array(maxis)\n",
    "    indices = maxis.argsort()[-5:][::-1]\n",
    "    #print('Highest acc at')\n",
    "    #print(indices)\n",
    "    parents= []\n",
    "    \n",
    "   \n",
    "    for ind in indices:    \n",
    "        parents.append(actualpop[ind])\n",
    "    \n",
    "    random.shuffle(parents)\n",
    "    f= 0\n",
    "   \n",
    "    while f < len(parents)-1: \n",
    "        mom = parents[f]\n",
    "        f= f +1\n",
    "        dad = parents[f]     \n",
    "        child = paring(mom,dad)\n",
    "        childs.append(child)\n",
    "        child = paring(dad,mom)\n",
    "        childs.append(child)\n",
    "        \n",
    "        \n",
    "    return childs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def netzdurchlauf(acutalpop):\n",
    "    accuracy_of_population= []\n",
    "    \n",
    "    print('Aktuelle Population')\n",
    "    print(actualpop)\n",
    "    for element in actualpop:\n",
    "        batch_size = 128\n",
    "        num_classes = 10\n",
    "        epochs = 12\n",
    "\n",
    "        # input image dimensions\n",
    "        img_rows, img_cols = 28, 28\n",
    "\n",
    "        # the data, split between train and test sets\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "            x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "            input_shape = (1, img_rows, img_cols)\n",
    "        else:\n",
    "            x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "            x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "            input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "#             print('x_train shape:', x_train.shape)\n",
    "#             print(x_train.shape[0], 'train samples')\n",
    "#             print(x_test.shape[0], 'test samples')\n",
    "\n",
    "        # convert class vectors to binary class matrices\n",
    "        y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "        y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "            ########\n",
    "            \n",
    "            \n",
    "\n",
    "        for gen in element:\n",
    "            if (gen== 0):\n",
    "                model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "            elif (gen == 1):\n",
    "                model.add(Dense(128, activation='relu'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                          optimizer=keras.optimizers.Adadelta(),\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "        model.fit(x_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      verbose=1,\n",
    "                      validation_data=(x_test, y_test))\n",
    "        score = model.evaluate(x_test, y_test, verbose=0)\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "        accuracy_of_population.append(score[1])\n",
    "        \n",
    "    return (accuracy_of_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fortpflanzung (maxis, actualpop):\n",
    "    childs= []\n",
    "    maxis= np.array(maxis)\n",
    "    indices = maxis.argsort()[-4:][::-1] # hier verändern, wenn wir individuen anzahl erhöhen die erste zahl muss die hälfte der anzahl sein\n",
    "    #print('Highest acc at')\n",
    "    #print(indices)\n",
    "    parents= []\n",
    "    \n",
    "    \n",
    "    for ind in indices:    \n",
    "        parents.append(actualpop[ind])\n",
    "   \n",
    "    \n",
    "\n",
    "    random.shuffle(parents)\n",
    "    f= 0\n",
    " \n",
    "    \n",
    "    while f < len(parents): \n",
    "        #print(f)\n",
    "        f = f -1\n",
    "        mom = parents[f]\n",
    "        f= f +1\n",
    "        #print(f)\n",
    "        dad = parents[f]    \n",
    "        f= f +1\n",
    "        decider = random.uniform(0, 1) # decides random how the cross-over works\n",
    "        if decider < 0.5 :\n",
    "            child = paring(mom,dad)\n",
    "            childs.append(child)\n",
    "        else:\n",
    "            child = paring(dad,mom)\n",
    "            childs.append(child)\n",
    "\n",
    "    \n",
    "\n",
    "    parents = np.concatenate((parents, childs), axis=0) \n",
    "    \n",
    " \n",
    "    return parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aktuelle Population\n",
      "[array([0, 1, 0, 0, 0, 1]), array([1, 1, 1, 1, 1, 0]), array([0, 1, 1, 0, 0, 1]), array([1, 1, 1, 0, 1, 0]), array([1, 0, 1, 1, 0, 1]), array([1, 0, 0, 1, 0, 1]), array([1, 1, 1, 1, 0, 1]), array([0, 1, 0, 1, 0, 0])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0927 07:48:50.566140 16712 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0927 07:48:50.592072 16712 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0927 07:48:50.601026 16712 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0927 07:48:50.629956 16712 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0927 07:48:50.631948 16712 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0927 07:48:50.637935 16712 deprecation.py:506] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0927 07:48:50.769580 16712 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0927 07:48:50.776564 16712 deprecation_wrapper.py:119] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0927 07:48:50.928158 16712 deprecation.py:323] From C:\\Users\\vivia\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 16s 268us/step - loss: 0.3679 - acc: 0.8787 - val_loss: 0.0667 - val_acc: 0.9796\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 9s 148us/step - loss: 0.0799 - acc: 0.9745 - val_loss: 0.0431 - val_acc: 0.9867\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.0522 - acc: 0.9841 - val_loss: 0.0344 - val_acc: 0.9893\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 0.0411 - acc: 0.9871 - val_loss: 0.0310 - val_acc: 0.9904\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0333 - acc: 0.9898 - val_loss: 0.0280 - val_acc: 0.9914\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.0266 - acc: 0.9914 - val_loss: 0.0237 - val_acc: 0.9934\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: 0.0235 - acc: 0.9930 - val_loss: 0.0213 - val_acc: 0.9926\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0210 - acc: 0.9935 - val_loss: 0.0201 - val_acc: 0.9933\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.0172 - acc: 0.9945 - val_loss: 0.0296 - val_acc: 0.9927\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0159 - acc: 0.9953 - val_loss: 0.0292 - val_acc: 0.9912\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0136 - acc: 0.9958 - val_loss: 0.0199 - val_acc: 0.9939\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 10s 173us/step - loss: 0.0131 - acc: 0.9958 - val_loss: 0.0243 - val_acc: 0.9932\n",
      "Test loss: 0.024278590999629524\n",
      "Test accuracy: 0.9932\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 15s 255us/step - loss: 0.2815 - acc: 0.9126 - val_loss: 0.0740 - val_acc: 0.9767\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 15s 244us/step - loss: 0.0949 - acc: 0.9717 - val_loss: 0.0523 - val_acc: 0.9840loss: 0.0950 - acc: 0.9\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 14s 241us/step - loss: 0.0745 - acc: 0.9779 - val_loss: 0.0434 - val_acc: 0.9857\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0634 - acc: 0.9810 - val_loss: 0.0395 - val_acc: 0.9871\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.0545 - acc: 0.9831 - val_loss: 0.0391 - val_acc: 0.9885\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0490 - acc: 0.9855 - val_loss: 0.0330 - val_acc: 0.9896\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.0441 - acc: 0.9860 - val_loss: 0.0337 - val_acc: 0.9887\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0416 - acc: 0.9870 - val_loss: 0.0340 - val_acc: 0.9891\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0383 - acc: 0.9880 - val_loss: 0.0306 - val_acc: 0.9891\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.0370 - acc: 0.9886 - val_loss: 0.0299 - val_acc: 0.9898\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0352 - acc: 0.9892 - val_loss: 0.0367 - val_acc: 0.9894\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0325 - acc: 0.9900 - val_loss: 0.0341 - val_acc: 0.9884\n",
      "Test loss: 0.03409686452727765\n",
      "Test accuracy: 0.9884\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.3007 - acc: 0.9025 - val_loss: 0.0551 - val_acc: 0.9824\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 176us/step - loss: 0.0700 - acc: 0.9785 - val_loss: 0.0337 - val_acc: 0.9894\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0487 - acc: 0.9855 - val_loss: 0.0246 - val_acc: 0.9913\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0377 - acc: 0.9884 - val_loss: 0.0273 - val_acc: 0.9915\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0325 - acc: 0.9899 - val_loss: 0.0292 - val_acc: 0.9918\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0281 - acc: 0.9919 - val_loss: 0.0244 - val_acc: 0.9921\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0243 - acc: 0.9924 - val_loss: 0.0271 - val_acc: 0.9927\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0220 - acc: 0.9932 - val_loss: 0.0195 - val_acc: 0.9947\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.0183 - acc: 0.9945 - val_loss: 0.0223 - val_acc: 0.9933\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 10s 175us/step - loss: 0.0177 - acc: 0.9941 - val_loss: 0.0300 - val_acc: 0.9918\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 180us/step - loss: 0.0158 - acc: 0.9947 - val_loss: 0.0237 - val_acc: 0.9931\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0154 - acc: 0.9953 - val_loss: 0.0227 - val_acc: 0.9931 loss: 0\n",
      "Test loss: 0.022740725728314648\n",
      "Test accuracy: 0.9931\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 15s 242us/step - loss: 0.2591 - acc: 0.9194 - val_loss: 0.0541 - val_acc: 0.9822\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 14s 234us/step - loss: 0.0778 - acc: 0.9759 - val_loss: 0.0408 - val_acc: 0.9868\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0566 - acc: 0.9825 - val_loss: 0.0319 - val_acc: 0.9891\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 0.0464 - acc: 0.9859 - val_loss: 0.0338 - val_acc: 0.9884\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0398 - acc: 0.9877 - val_loss: 0.0302 - val_acc: 0.9897\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.0353 - acc: 0.9893 - val_loss: 0.0310 - val_acc: 0.9892\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 14s 231us/step - loss: 0.0304 - acc: 0.9903 - val_loss: 0.0268 - val_acc: 0.9908\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.0283 - acc: 0.9911 - val_loss: 0.0253 - val_acc: 0.9917\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.0261 - acc: 0.9915 - val_loss: 0.0242 - val_acc: 0.9923\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0234 - acc: 0.9922 - val_loss: 0.0262 - val_acc: 0.9910\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.0227 - acc: 0.9925 - val_loss: 0.0240 - val_acc: 0.9921\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 13s 217us/step - loss: 0.0207 - acc: 0.9933 - val_loss: 0.0291 - val_acc: 0.9905\n",
      "Test loss: 0.029109642832372627\n",
      "Test accuracy: 0.9905\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 218us/step - loss: 0.2784 - acc: 0.9135 - val_loss: 0.0649 - val_acc: 0.9821\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 12s 206us/step - loss: 0.0736 - acc: 0.9780 - val_loss: 0.0528 - val_acc: 0.9849\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 12s 207us/step - loss: 0.0531 - acc: 0.9841 - val_loss: 0.0403 - val_acc: 0.9858\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 0.0424 - acc: 0.9873 - val_loss: 0.0306 - val_acc: 0.9898\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 0.0365 - acc: 0.9887 - val_loss: 0.0264 - val_acc: 0.9908\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.0309 - acc: 0.9905 - val_loss: 0.0272 - val_acc: 0.9901\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.0282 - acc: 0.9911 - val_loss: 0.0262 - val_acc: 0.9911\n",
      "Epoch 8/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.0243 - acc: 0.9919 - val_loss: 0.0247 - val_acc: 0.9916\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 0.0225 - acc: 0.9931 - val_loss: 0.0284 - val_acc: 0.9910\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.0211 - acc: 0.9935 - val_loss: 0.0210 - val_acc: 0.9932\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.0191 - acc: 0.9941 - val_loss: 0.0252 - val_acc: 0.9918\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.0174 - acc: 0.9945 - val_loss: 0.0236 - val_acc: 0.9925\n",
      "Test loss: 0.023597840967782394\n",
      "Test accuracy: 0.9925\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 210us/step - loss: 0.3101 - acc: 0.9009 - val_loss: 0.0597 - val_acc: 0.9814\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0726 - acc: 0.9776 - val_loss: 0.0370 - val_acc: 0.9873\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0488 - acc: 0.9854 - val_loss: 0.0340 - val_acc: 0.9885\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.0401 - acc: 0.9875 - val_loss: 0.0266 - val_acc: 0.9916\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0342 - acc: 0.9892 - val_loss: 0.0266 - val_acc: 0.9912\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0278 - acc: 0.9912 - val_loss: 0.0258 - val_acc: 0.9927\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0244 - acc: 0.9922 - val_loss: 0.0243 - val_acc: 0.9928\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0218 - acc: 0.9928 - val_loss: 0.0267 - val_acc: 0.9915\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0197 - acc: 0.9939 - val_loss: 0.0208 - val_acc: 0.9938\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0180 - acc: 0.9940 - val_loss: 0.0197 - val_acc: 0.9942\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0164 - acc: 0.9950 - val_loss: 0.0216 - val_acc: 0.9929\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0144 - acc: 0.9956 - val_loss: 0.0204 - val_acc: 0.9941\n",
      "Test loss: 0.020393489237905668\n",
      "Test accuracy: 0.9941\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 0.2431 - acc: 0.9249 - val_loss: 0.0623 - val_acc: 0.9810\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 13s 208us/step - loss: 0.0778 - acc: 0.9768 - val_loss: 0.0531 - val_acc: 0.9830\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0592 - acc: 0.9822 - val_loss: 0.0378 - val_acc: 0.9880\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0488 - acc: 0.9848 - val_loss: 0.0421 - val_acc: 0.9863\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 0.0434 - acc: 0.9871 - val_loss: 0.0352 - val_acc: 0.9881\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.0389 - acc: 0.9883 - val_loss: 0.0357 - val_acc: 0.9876\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.0343 - acc: 0.9889 - val_loss: 0.0328 - val_acc: 0.9891\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0320 - acc: 0.9893 - val_loss: 0.0342 - val_acc: 0.9886\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0281 - acc: 0.9910 - val_loss: 0.0304 - val_acc: 0.9898\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0265 - acc: 0.9918 - val_loss: 0.0312 - val_acc: 0.9899\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 14s 225us/step - loss: 0.0255 - acc: 0.9920 - val_loss: 0.0269 - val_acc: 0.9916\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.0232 - acc: 0.9928 - val_loss: 0.0276 - val_acc: 0.9916\n",
      "Test loss: 0.02760617357788433\n",
      "Test accuracy: 0.9916\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.3771 - acc: 0.8775 - val_loss: 0.0774 - val_acc: 0.9742\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.0803 - acc: 0.9751 - val_loss: 0.0442 - val_acc: 0.9867\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0544 - acc: 0.9830 - val_loss: 0.0312 - val_acc: 0.9897\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0427 - acc: 0.9868 - val_loss: 0.0291 - val_acc: 0.9914\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.0355 - acc: 0.9884 - val_loss: 0.0236 - val_acc: 0.9927\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 0.0290 - acc: 0.9911 - val_loss: 0.0237 - val_acc: 0.9922\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0252 - acc: 0.9920 - val_loss: 0.0226 - val_acc: 0.9922\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0228 - acc: 0.9930 - val_loss: 0.0188 - val_acc: 0.9936\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0200 - acc: 0.9937 - val_loss: 0.0216 - val_acc: 0.9931\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.0183 - acc: 0.9940 - val_loss: 0.0250 - val_acc: 0.9922\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.0162 - acc: 0.9948 - val_loss: 0.0202 - val_acc: 0.9936\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.0154 - acc: 0.9950 - val_loss: 0.0221 - val_acc: 0.9932\n",
      "Test loss: 0.02208645986196061\n",
      "Test accuracy: 0.9932\n",
      " Accuracy of populationnummer: \n",
      "1\n",
      "[0.9932, 0.9884, 0.9931, 0.9905, 0.9925, 0.9941, 0.9916, 0.9932]\n",
      "highest accuracy of populationnummer: \n",
      "1\n",
      "0.9941\n",
      "Muation\n",
      "Muation\n",
      "Muation\n",
      "Aktuelle Population\n",
      "[[0 1 1 0 0 1]\n",
      " [0 1 0 0 1 1]\n",
      " [1 0 0 1 0 1]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 0 1 0 1]\n",
      " [0 1 0 0 0 1]\n",
      " [0 1 0 1 0 1]\n",
      " [1 0 1 1 0 0]]\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 213us/step - loss: 0.2903 - acc: 0.9064 - val_loss: 0.0766 - val_acc: 0.9768\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.0726 - acc: 0.9778 - val_loss: 0.0344 - val_acc: 0.9884\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.0472 - acc: 0.9853 - val_loss: 0.0362 - val_acc: 0.9877\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 12s 202us/step - loss: 0.0397 - acc: 0.9872 - val_loss: 0.0274 - val_acc: 0.9913\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.0328 - acc: 0.9899 - val_loss: 0.0266 - val_acc: 0.9912\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0287 - acc: 0.9909 - val_loss: 0.0245 - val_acc: 0.9915\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0244 - acc: 0.9921 - val_loss: 0.0212 - val_acc: 0.9934\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 12s 200us/step - loss: 0.0219 - acc: 0.9931 - val_loss: 0.0199 - val_acc: 0.9928\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.0196 - acc: 0.9941 - val_loss: 0.0194 - val_acc: 0.9939\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.0172 - acc: 0.9942 - val_loss: 0.0190 - val_acc: 0.9935\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0153 - acc: 0.9950 - val_loss: 0.0205 - val_acc: 0.9928\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0143 - acc: 0.9956 - val_loss: 0.0207 - val_acc: 0.9935\n",
      "Test loss: 0.020695028959176125\n",
      "Test accuracy: 0.9935\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 13s 209us/step - loss: 0.3090 - acc: 0.9006 - val_loss: 0.0734 - val_acc: 0.9762\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0783 - acc: 0.9765 - val_loss: 0.0624 - val_acc: 0.9796\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: 0.0537 - acc: 0.9834 - val_loss: 0.0301 - val_acc: 0.9892\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0409 - acc: 0.9869 - val_loss: 0.0311 - val_acc: 0.9903\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 12s 198us/step - loss: 0.0342 - acc: 0.9898 - val_loss: 0.0241 - val_acc: 0.9917\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0284 - acc: 0.9912 - val_loss: 0.0302 - val_acc: 0.9888\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0258 - acc: 0.9920 - val_loss: 0.0231 - val_acc: 0.9927\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0224 - acc: 0.9932 - val_loss: 0.0207 - val_acc: 0.9935\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: 0.0189 - acc: 0.9943 - val_loss: 0.0249 - val_acc: 0.9929\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.0183 - acc: 0.9942 - val_loss: 0.0229 - val_acc: 0.9917\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0171 - acc: 0.9942 - val_loss: 0.0221 - val_acc: 0.9930\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0145 - acc: 0.9952 - val_loss: 0.0247 - val_acc: 0.9924\n",
      "Test loss: 0.02469892613327161\n",
      "Test accuracy: 0.9924\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.2941 - acc: 0.9057 - val_loss: 0.0630 - val_acc: 0.9813\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 0.0740 - acc: 0.9773 - val_loss: 0.0384 - val_acc: 0.9879\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0516 - acc: 0.9844 - val_loss: 0.0298 - val_acc: 0.9900\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.0401 - acc: 0.9876 - val_loss: 0.0242 - val_acc: 0.9928\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0324 - acc: 0.9899 - val_loss: 0.0259 - val_acc: 0.9919\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0302 - acc: 0.9908 - val_loss: 0.0226 - val_acc: 0.9922\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0249 - acc: 0.9922 - val_loss: 0.0234 - val_acc: 0.9930\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0227 - acc: 0.9931 - val_loss: 0.0248 - val_acc: 0.9921\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0194 - acc: 0.9936 - val_loss: 0.0223 - val_acc: 0.9933\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0179 - acc: 0.9944 - val_loss: 0.0207 - val_acc: 0.9933\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.0159 - acc: 0.9951 - val_loss: 0.0262 - val_acc: 0.9923\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.0154 - acc: 0.9948 - val_loss: 0.0258 - val_acc: 0.9931\n",
      "Test loss: 0.02583416008528243\n",
      "Test accuracy: 0.9931\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 13s 214us/step - loss: 0.3869 - acc: 0.8750 - val_loss: 0.0808 - val_acc: 0.9749\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0870 - acc: 0.9737 - val_loss: 0.0360 - val_acc: 0.9883\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 12s 204us/step - loss: 0.0567 - acc: 0.9826 - val_loss: 0.0418 - val_acc: 0.9872\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 12s 203us/step - loss: 0.0425 - acc: 0.9869 - val_loss: 0.0381 - val_acc: 0.9881\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0364 - acc: 0.9890 - val_loss: 0.0248 - val_acc: 0.9922\n",
      "Epoch 6/12\n",
      "16640/60000 [=======>......................] - ETA: 7s - loss: 0.0307 - acc: 0.9903"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c75511e26423>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mnummer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0maccuri\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetzdurchlauf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactualpop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' Accuracy of populationnummer: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnummer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-7c7ca8e6cede>\u001b[0m in \u001b[0;36mnetzdurchlauf\u001b[1;34m(acutalpop)\u001b[0m\n\u001b[0;32m     70\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                       \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m                       validation_data=(x_test, y_test))\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test loss:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Hauptmethode\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "DURCHGÄNGE = 10 # variable to clarify number of generations\n",
    "actualpop= initialise()\n",
    "\n",
    "#actualpop= entschachteln(actualpop)\n",
    "evalaccuris = []\n",
    "saved = actualpop\n",
    "i = 1\n",
    "\n",
    "    \n",
    "while i < DURCHGÄNGE+1:\n",
    "    \n",
    "    nummer = i\n",
    "    accuri = netzdurchlauf(actualpop)\n",
    "    print(' Accuracy of populationnummer: ')\n",
    "    print( nummer)\n",
    "    print(accuri)\n",
    "    print('highest accuracy of populationnummer: ')\n",
    "    print( nummer)\n",
    "    print(np.amax(accuri))\n",
    "    #print(actualpop)\n",
    "    actualpop = fortpflanzung(accuri, actualpop)\n",
    "    actualpop = mutationeach(actualpop)\n",
    "   \n",
    "    evalaccuris.append([i , sum(accuri)/len(accuri),np.amax(accuri),actualpop[np.argmax(accuri)]])\n",
    "    i = i+1\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print('Wartezeit: ')\n",
    "end = time.time()\n",
    "seconds = end - start\n",
    "minutes = seconds / 60\n",
    "print(minutes)\n",
    "hours = minutes/60\n",
    "print(hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation\n",
    "df = pd.DataFrame(data =evalaccuris)\n",
    "df.rename(columns={0: 'Generation',1: 'Accuracy Mean',2:'Accuracy Best',3: 'Best Individum'}, inplace=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr =sns.regplot(x=df['Generation'],y=df['Accuracy Best'])\n",
    "plt.ylim(0.990, 1)\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig = fr.get_figure()\n",
    "fig.savefig('Best_Accuracies_in_Generation ( M: 0.15).png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation von dem durchschnitt der accuracy\n",
    "\n",
    "fr =sns.regplot(x=df['Generation'],y=df['Accuracy Mean'])\n",
    "plt.ylim(0.990, 1)\n",
    "\n",
    "fig = fr.get_figure()\n",
    "fig.savefig('Average_Accuracy_in_Generation ( M: 0.15).png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "ax1 =sns.regplot(x=df['Generation'],y=df['Accuracy Best'])\n",
    "ax0 =sns.regplot(x=df['Generation'],y=df['Accuracy Mean'])\n",
    "fig = fr.get_figure()\n",
    "fig.savefig('Accuracies Mutation: 3.35.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wievielschwnakung \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rettung=evalaccuris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evalaccuris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1]\n",
      " [0 1 0 0 1 1]\n",
      " [1 0 0 1 0 1]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 0 1 0 1]\n",
      " [0 1 0 0 0 1]\n",
      " [0 1 0 1 0 1]\n",
      " [1 0 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(actualpop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def craschcheck(popul):\n",
    "    i=0\n",
    "    for indi in popul:\n",
    "        if check(indi) == True:\n",
    "            continue;\n",
    "        if check(indi) == False:\n",
    "            indi[2] = 1\n",
    "            print('Problem')\n",
    "            popul[i] = np.array(indi)\n",
    "        \n",
    "            \n",
    "        i = i +1\n",
    "                \n",
    "\n",
    "            \n",
    "def check(indi):\n",
    "    for gen in indi:\n",
    "        if gen == 1:\n",
    "            return True\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "Problem\n",
      "Problem\n",
      "[array([0, 0, 1, 0, 0, 0]), array([0, 0, 1, 0, 0, 0, 0, 0, 0]), [0, 0, 1, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "la = [[0, 0, 0, 0, 0, 0],[1,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0]]\n",
    "\n",
    "print (la)\n",
    "\n",
    "craschcheck(la)\n",
    "print(la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
